{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Proyecto 1: AnÃ¡lisis Detallado de MÃ©tricas\n",
        "## EvaluaciÃ³n Completa del Modelo Final\n",
        "\n",
        "**Objetivo**: Generar un anÃ¡lisis completo de mÃ©tricas incluyendo matriz de confusiÃ³n, curva ROC, AUC y reportes de clasificaciÃ³n detallados para el mejor modelo seleccionado.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ImportaciÃ³n de bibliotecas para anÃ¡lisis de mÃ©tricas\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import joblib\n",
        "from sklearn.metrics import (classification_report, confusion_matrix, \n",
        "                           roc_curve, auc, roc_auc_score,\n",
        "                           precision_recall_curve, average_precision_score)\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from itertools import cycle\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ConfiguraciÃ³n de visualizaciÃ³n\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"âœ… Bibliotecas para anÃ¡lisis de mÃ©tricas importadas correctamente\")\n",
        "print(\"ğŸ“Š ConfiguraciÃ³n de visualizaciÃ³n establecida\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. Carga del Modelo y Datos de Prueba\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cargar modelo y datos necesarios\n",
        "try:\n",
        "    # Cargar datos preprocesados\n",
        "    processed_datasets = joblib.load('../models/processed_datasets.joblib')\n",
        "    label_encoder = joblib.load('../models/label_encoder.joblib')\n",
        "    test_results = joblib.load('../models/test_results.joblib')\n",
        "    \n",
        "    # Obtener el nombre del mejor modelo\n",
        "    best_model_name = test_results['model_name']\n",
        "    model_filename = f'../models/best_model_{best_model_name.replace(\" \", \"_\")}.joblib'\n",
        "    best_model = joblib.load(model_filename)\n",
        "    \n",
        "    print(\"âœ… Modelo y datos cargados correctamente\")\n",
        "    print(f\"ğŸ† Mejor modelo: {best_model_name}\")\n",
        "    \n",
        "except FileNotFoundError as e:\n",
        "    print(f\"âŒ Error: {e}\")\n",
        "    print(\"ğŸ“‹ AsegÃºrate de ejecutar primero los notebooks de preprocesamiento y benchmarking\")\n",
        "\n",
        "# Obtener datos de prueba del dataset principal\n",
        "main_dataset = processed_datasets['StandardScaler_OneHot']\n",
        "X_test = main_dataset['X_test']\n",
        "y_test = main_dataset['y_test']\n",
        "\n",
        "# Hacer predicciones\n",
        "y_pred = best_model.predict(X_test)\n",
        "y_pred_proba = best_model.predict_proba(X_test)\n",
        "\n",
        "print(f\"\\nğŸ“Š INFORMACIÃ“N DEL CONJUNTO DE PRUEBA:\")\n",
        "print(f\"   ğŸ§ª TamaÃ±o: {X_test.shape[0]} muestras\")\n",
        "print(f\"   ğŸ“‹ Features: {X_test.shape[1]}\")\n",
        "print(f\"   ğŸ¯ Clases: {len(label_encoder.classes_)}\")\n",
        "\n",
        "print(f\"\\nğŸ”® PREDICCIONES GENERADAS:\")\n",
        "print(f\"   âœ… Predicciones discretas: {y_pred.shape}\")\n",
        "print(f\"   ğŸ“Š Probabilidades: {y_pred_proba.shape}\")\n",
        "\n",
        "# Mostrar distribuciÃ³n de predicciones\n",
        "pred_distribution = pd.Series(y_pred).value_counts().sort_index()\n",
        "print(f\"\\nğŸ“Š DISTRIBUCIÃ“N DE PREDICCIONES:\")\n",
        "for class_idx, count in pred_distribution.items():\n",
        "    class_name = label_encoder.inverse_transform([class_idx])[0]\n",
        "    percentage = (count / len(y_pred)) * 100\n",
        "    print(f\"   {class_name}: {count} ({percentage:.1f}%)\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. Reporte de ClasificaciÃ³n Detallado\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generar reporte de clasificaciÃ³n detallado\n",
        "class_names = label_encoder.classes_\n",
        "\n",
        "print(\"ğŸ“‹ REPORTE DE CLASIFICACIÃ“N DETALLADO\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Reporte de clasificaciÃ³n con nombres de clases\n",
        "classification_rep = classification_report(\n",
        "    y_test, y_pred, \n",
        "    target_names=class_names,\n",
        "    output_dict=True\n",
        ")\n",
        "\n",
        "# Mostrar reporte formateado\n",
        "print(classification_report(y_test, y_pred, target_names=class_names))\n",
        "\n",
        "# Crear DataFrame para mejor anÃ¡lisis\n",
        "report_df = pd.DataFrame(classification_rep).transpose()\n",
        "report_df = report_df.round(4)\n",
        "\n",
        "print(\"\\nğŸ“Š MÃ‰TRICAS POR CLASE:\")\n",
        "print(\"=\"*50)\n",
        "for class_name in class_names:\n",
        "    if class_name in report_df.index:\n",
        "        precision = report_df.loc[class_name, 'precision']\n",
        "        recall = report_df.loc[class_name, 'recall']\n",
        "        f1 = report_df.loc[class_name, 'f1-score']\n",
        "        support = int(report_df.loc[class_name, 'support'])\n",
        "        \n",
        "        print(f\"\\nğŸ·ï¸ Clase: {class_name}\")\n",
        "        print(f\"   ğŸ“ˆ Precision: {precision:.4f}\")\n",
        "        print(f\"   ğŸ“ˆ Recall: {recall:.4f}\")\n",
        "        print(f\"   ğŸ“ˆ F1-Score: {f1:.4f}\")\n",
        "        print(f\"   ğŸ“Š Support: {support} muestras\")\n",
        "\n",
        "# MÃ©tricas globales\n",
        "macro_avg = report_df.loc['macro avg']\n",
        "weighted_avg = report_df.loc['weighted avg']\n",
        "\n",
        "print(f\"\\nğŸŒ MÃ‰TRICAS GLOBALES:\")\n",
        "print(f\"   ğŸ“Š Macro Average:\")\n",
        "print(f\"      â€¢ Precision: {macro_avg['precision']:.4f}\")\n",
        "print(f\"      â€¢ Recall: {macro_avg['recall']:.4f}\")\n",
        "print(f\"      â€¢ F1-Score: {macro_avg['f1-score']:.4f}\")\n",
        "print(f\"   ğŸ“Š Weighted Average:\")\n",
        "print(f\"      â€¢ Precision: {weighted_avg['precision']:.4f}\")\n",
        "print(f\"      â€¢ Recall: {weighted_avg['recall']:.4f}\")\n",
        "print(f\"      â€¢ F1-Score: {weighted_avg['f1-score']:.4f}\")\n",
        "\n",
        "# Guardar reporte de clasificaciÃ³n\n",
        "report_df.to_csv('../reports/classification_report.csv')\n",
        "print(f\"\\nğŸ’¾ Reporte guardado: ../reports/classification_report.csv\")\n",
        "\n",
        "# Guardar reporte de clasificaciÃ³n en texto\n",
        "with open('../reports/classification_report.txt', 'w') as f:\n",
        "    f.write(f\"Reporte de ClasificaciÃ³n - Modelo: {best_model_name}\\n\")\n",
        "    f.write(\"=\"*60 + \"\\n\\n\")\n",
        "    f.write(classification_report(y_test, y_pred, target_names=class_names))\n",
        "\n",
        "print(f\"ğŸ’¾ Reporte en texto guardado: ../reports/classification_report.txt\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. Matriz de ConfusiÃ³n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear y visualizar la matriz de confusiÃ³n\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Crear visualizaciÃ³n de la matriz de confusiÃ³n\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "fig.suptitle(f'ğŸ“Š MATRIZ DE CONFUSIÃ“N - {best_model_name}\\nğŸ¯ AnÃ¡lisis de Errores de ClasificaciÃ³n', \n",
        "             fontsize=16, fontweight='bold', y=1.02)\n",
        "\n",
        "# Matriz de confusiÃ³n en nÃºmeros absolutos\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=class_names, yticklabels=class_names,\n",
        "            ax=axes[0], cbar_kws={'shrink': 0.8})\n",
        "axes[0].set_title('ğŸ“ˆ Valores Absolutos', fontweight='bold', fontsize=14)\n",
        "axes[0].set_xlabel('PredicciÃ³n')\n",
        "axes[0].set_ylabel('Valor Real')\n",
        "\n",
        "# Matriz de confusiÃ³n normalizada (porcentajes)\n",
        "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Oranges',\n",
        "            xticklabels=class_names, yticklabels=class_names,\n",
        "            ax=axes[1], cbar_kws={'shrink': 0.8})\n",
        "axes[1].set_title('ğŸ“Š Porcentajes (Normalizado)', fontweight='bold', fontsize=14)\n",
        "axes[1].set_xlabel('PredicciÃ³n')\n",
        "axes[1].set_ylabel('Valor Real')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('../reports/confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# AnÃ¡lisis detallado de la matriz de confusiÃ³n\n",
        "print(\"ğŸ“Š ANÃLISIS DETALLADO DE LA MATRIZ DE CONFUSIÃ“N\")\n",
        "print(\"=\"*55)\n",
        "\n",
        "total_samples = cm.sum()\n",
        "correct_predictions = np.trace(cm)\n",
        "accuracy = correct_predictions / total_samples\n",
        "\n",
        "print(f\"ğŸ¯ ESTADÃSTICAS GENERALES:\")\n",
        "print(f\"   ğŸ“Š Total de muestras: {total_samples}\")\n",
        "print(f\"   âœ… Predicciones correctas: {correct_predictions}\")\n",
        "print(f\"   âŒ Predicciones incorrectas: {total_samples - correct_predictions}\")\n",
        "print(f\"   ğŸ“ˆ Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
        "\n",
        "print(f\"\\nğŸ” ANÃLISIS POR CLASE:\")\n",
        "for i, class_name in enumerate(class_names):\n",
        "    # Verdaderos positivos, falsos positivos, falsos negativos\n",
        "    tp = cm[i, i]\n",
        "    fp = cm[:, i].sum() - tp\n",
        "    fn = cm[i, :].sum() - tp\n",
        "    tn = total_samples - tp - fp - fn\n",
        "    \n",
        "    # MÃ©tricas por clase\n",
        "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "    \n",
        "    print(f\"\\nğŸ·ï¸ Clase: {class_name}\")\n",
        "    print(f\"   âœ… Verdaderos Positivos (TP): {tp}\")\n",
        "    print(f\"   âŒ Falsos Positivos (FP): {fp}\")\n",
        "    print(f\"   âŒ Falsos Negativos (FN): {fn}\")\n",
        "    print(f\"   âœ… Verdaderos Negativos (TN): {tn}\")\n",
        "    print(f\"   ğŸ“ˆ Precision: {precision:.4f}\")\n",
        "    print(f\"   ğŸ“ˆ Recall (Sensibilidad): {recall:.4f}\")\n",
        "    print(f\"   ğŸ“ˆ Especificidad: {specificity:.4f}\")\n",
        "\n",
        "# Identificar errores mÃ¡s comunes\n",
        "print(f\"\\nâŒ ERRORES MÃS COMUNES:\")\n",
        "print(\"=\"*30)\n",
        "\n",
        "# Encontrar las confusiones mÃ¡s frecuentes (excluyendo la diagonal)\n",
        "errors = []\n",
        "for i in range(len(class_names)):\n",
        "    for j in range(len(class_names)):\n",
        "        if i != j and cm[i, j] > 0:\n",
        "            errors.append((class_names[i], class_names[j], cm[i, j]))\n",
        "\n",
        "# Ordenar por frecuencia de error\n",
        "errors.sort(key=lambda x: x[2], reverse=True)\n",
        "\n",
        "for true_class, pred_class, count in errors[:5]:  # Top 5 errores\n",
        "    percentage = (count / total_samples) * 100\n",
        "    print(f\"   ğŸ”„ {true_class} â†’ {pred_class}: {count} veces ({percentage:.2f}%)\")\n",
        "\n",
        "# Guardar matriz de confusiÃ³n\n",
        "np.savetxt('../reports/confusion_matrix.csv', cm, delimiter=',', fmt='%d')\n",
        "print(f\"\\nğŸ’¾ Matriz de confusiÃ³n guardada: ../reports/confusion_matrix.csv\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 4. Curvas ROC y AUC\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calcular curvas ROC y AUC para clasificaciÃ³n multiclase\n",
        "n_classes = len(class_names)\n",
        "\n",
        "# Binarizar las etiquetas para ROC multiclase\n",
        "y_test_binarized = label_binarize(y_test, classes=range(n_classes))\n",
        "\n",
        "# Si solo hay 2 clases, label_binarize devuelve un array 1D\n",
        "if n_classes == 2:\n",
        "    y_test_binarized = np.column_stack([1 - y_test_binarized, y_test_binarized])\n",
        "\n",
        "print(\"ğŸ“ˆ ANÃLISIS DE CURVAS ROC Y AUC\")\n",
        "print(\"=\"*40)\n",
        "\n",
        "# Calcular ROC para cada clase\n",
        "fpr = dict()\n",
        "tpr = dict()\n",
        "roc_auc = dict()\n",
        "\n",
        "for i in range(n_classes):\n",
        "    fpr[i], tpr[i], _ = roc_curve(y_test_binarized[:, i], y_pred_proba[:, i])\n",
        "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "# Calcular micro-average ROC\n",
        "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test_binarized.ravel(), y_pred_proba.ravel())\n",
        "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
        "\n",
        "# Calcular macro-average ROC\n",
        "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
        "mean_tpr = np.zeros_like(all_fpr)\n",
        "for i in range(n_classes):\n",
        "    mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
        "mean_tpr /= n_classes\n",
        "\n",
        "fpr[\"macro\"] = all_fpr\n",
        "tpr[\"macro\"] = mean_tpr\n",
        "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
        "\n",
        "# Visualizar curvas ROC\n",
        "plt.figure(figsize=(14, 10))\n",
        "\n",
        "# Colores para cada clase\n",
        "colors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'red', 'green', 'purple'])\n",
        "\n",
        "# Plotear ROC para cada clase\n",
        "for i, color in zip(range(n_classes), colors):\n",
        "    plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
        "             label=f'ROC {class_names[i]} (AUC = {roc_auc[i]:.3f})')\n",
        "\n",
        "# Plotear micro y macro average\n",
        "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
        "         label=f'Micro-average ROC (AUC = {roc_auc[\"micro\"]:.3f})',\n",
        "         color='deeppink', linestyle=':', linewidth=3)\n",
        "\n",
        "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
        "         label=f'Macro-average ROC (AUC = {roc_auc[\"macro\"]:.3f})',\n",
        "         color='navy', linestyle=':', linewidth=3)\n",
        "\n",
        "# LÃ­nea de referencia (clasificador aleatorio)\n",
        "plt.plot([0, 1], [0, 1], 'k--', lw=2, label='Clasificador Aleatorio (AUC = 0.500)')\n",
        "\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('Tasa de Falsos Positivos (1 - Especificidad)', fontsize=12, fontweight='bold')\n",
        "plt.ylabel('Tasa de Verdaderos Positivos (Sensibilidad)', fontsize=12, fontweight='bold')\n",
        "plt.title(f'ğŸ“ˆ CURVAS ROC MULTICLASE - {best_model_name}\\\\nğŸ¯ Receiver Operating Characteristic', \n",
        "          fontsize=16, fontweight='bold', pad=20)\n",
        "plt.legend(loc=\"lower right\", fontsize=10)\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# AÃ±adir anotaciones\n",
        "plt.text(0.6, 0.2, f'Mejor Rendimiento:\\\\nMacro AUC = {roc_auc[\"macro\"]:.3f}\\\\nMicro AUC = {roc_auc[\"micro\"]:.3f}',\n",
        "         bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\", alpha=0.8),\n",
        "         fontsize=11, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('../reports/roc_curve.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Mostrar AUC por clase\n",
        "print(f\"ğŸ“Š AUC POR CLASE:\")\n",
        "for i, class_name in enumerate(class_names):\n",
        "    print(f\"   ğŸ·ï¸ {class_name}: {roc_auc[i]:.4f}\")\n",
        "\n",
        "print(f\"\\nğŸ“Š AUC PROMEDIO:\")\n",
        "print(f\"   ğŸ” Micro-average: {roc_auc['micro']:.4f}\")\n",
        "print(f\"   ğŸ” Macro-average: {roc_auc['macro']:.4f}\")\n",
        "\n",
        "# InterpretaciÃ³n de AUC\n",
        "macro_auc = roc_auc['macro']\n",
        "if macro_auc >= 0.9:\n",
        "    interpretation = \"Excelente\"\n",
        "elif macro_auc >= 0.8:\n",
        "    interpretation = \"Bueno\"\n",
        "elif macro_auc >= 0.7:\n",
        "    interpretation = \"Aceptable\"\n",
        "elif macro_auc >= 0.6:\n",
        "    interpretation = \"Pobre\"\n",
        "else:\n",
        "    interpretation = \"Muy Pobre\"\n",
        "\n",
        "print(f\"\\nğŸ’¡ INTERPRETACIÃ“N DEL RENDIMIENTO:\")\n",
        "print(f\"   ğŸ“ˆ AUC Macro: {macro_auc:.4f} â†’ {interpretation}\")\n",
        "\n",
        "# Guardar resultados ROC\n",
        "roc_results = {\n",
        "    'class_auc': {class_names[i]: roc_auc[i] for i in range(n_classes)},\n",
        "    'micro_auc': roc_auc['micro'],\n",
        "    'macro_auc': roc_auc['macro']\n",
        "}\n",
        "\n",
        "import json\n",
        "with open('../reports/roc_auc_results.json', 'w') as f:\n",
        "    json.dump(roc_results, f, indent=2)\n",
        "\n",
        "print(f\"\\nğŸ’¾ Resultados ROC guardados: ../reports/roc_auc_results.json\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 5. Resumen Final y Conclusiones\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generar resumen final completo\n",
        "print(\"ğŸ¯ RESUMEN FINAL DEL ANÃLISIS DE MÃ‰TRICAS\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "print(f\"\\nğŸ† MODELO SELECCIONADO: {best_model_name}\")\n",
        "print(f\"ğŸ“Š Dataset: ClasificaciÃ³n de categorÃ­as de venta (retail)\")\n",
        "print(f\"ğŸ¯ Problema: ClasificaciÃ³n multiclase ({n_classes} clases)\")\n",
        "\n",
        "print(f\"\\nğŸ“ˆ RENDIMIENTO EN CONJUNTO DE PRUEBA:\")\n",
        "print(f\"   ğŸ¯ Accuracy: {test_results['test_accuracy']:.4f}\")\n",
        "print(f\"   ğŸ“Š Precision (macro): {test_results['test_precision']:.4f}\")\n",
        "print(f\"   ğŸ“Š Recall (macro): {test_results['test_recall']:.4f}\")\n",
        "print(f\"   ğŸ“Š F1-Score (macro): {test_results['test_f1']:.4f}\")\n",
        "print(f\"   ğŸ“ˆ AUC (macro): {roc_auc['macro']:.4f}\")\n",
        "\n",
        "print(f\"\\nğŸ” ANÃLISIS POR CLASE:\")\n",
        "for i, class_name in enumerate(class_names):\n",
        "    if class_name in report_df.index:\n",
        "        precision = report_df.loc[class_name, 'precision']\n",
        "        recall = report_df.loc[class_name, 'recall']\n",
        "        f1 = report_df.loc[class_name, 'f1-score']\n",
        "        auc_score = roc_auc[i]\n",
        "        support = int(report_df.loc[class_name, 'support'])\n",
        "        \n",
        "        print(f\"   ğŸ·ï¸ {class_name}:\")\n",
        "        print(f\"      â€¢ Precision: {precision:.3f} | Recall: {recall:.3f} | F1: {f1:.3f} | AUC: {auc_score:.3f}\")\n",
        "        print(f\"      â€¢ Muestras: {support}\")\n",
        "\n",
        "print(f\"\\nğŸ’¡ INSIGHTS CLAVE:\")\n",
        "# Identificar la mejor y peor clase\n",
        "best_class_idx = np.argmax([roc_auc[i] for i in range(n_classes)])\n",
        "worst_class_idx = np.argmin([roc_auc[i] for i in range(n_classes)])\n",
        "\n",
        "best_class = class_names[best_class_idx]\n",
        "worst_class = class_names[worst_class_idx]\n",
        "\n",
        "print(f\"   ğŸ† Mejor clase predicha: {best_class} (AUC: {roc_auc[best_class_idx]:.3f})\")\n",
        "print(f\"   ğŸ“‰ Clase mÃ¡s difÃ­cil: {worst_class} (AUC: {roc_auc[worst_class_idx]:.3f})\")\n",
        "\n",
        "# AnÃ¡lisis de balance del dataset\n",
        "class_distribution = pd.Series(y_test).value_counts().sort_index()\n",
        "most_frequent_class = label_encoder.inverse_transform([class_distribution.idxmax()])[0]\n",
        "least_frequent_class = label_encoder.inverse_transform([class_distribution.idxmin()])[0]\n",
        "\n",
        "print(f\"   ğŸ“Š Clase mÃ¡s frecuente: {most_frequent_class} ({class_distribution.max()} muestras)\")\n",
        "print(f\"   ğŸ“Š Clase menos frecuente: {least_frequent_class} ({class_distribution.min()} muestras)\")\n",
        "\n",
        "# Evaluar si hay desbalance\n",
        "max_count = class_distribution.max()\n",
        "min_count = class_distribution.min()\n",
        "imbalance_ratio = max_count / min_count\n",
        "\n",
        "if imbalance_ratio > 2:\n",
        "    print(f\"   âš ï¸ Dataset desbalanceado (ratio: {imbalance_ratio:.1f}:1)\")\n",
        "else:\n",
        "    print(f\"   âœ… Dataset relativamente balanceado (ratio: {imbalance_ratio:.1f}:1)\")\n",
        "\n",
        "print(f\"\\nğŸ“‹ ARCHIVOS GENERADOS:\")\n",
        "print(f\"   ğŸ“„ ../reports/classification_report.csv\")\n",
        "print(f\"   ğŸ“„ ../reports/classification_report.txt\")\n",
        "print(f\"   ğŸ“Š ../reports/confusion_matrix.png\")\n",
        "print(f\"   ğŸ“Š ../reports/confusion_matrix.csv\")\n",
        "print(f\"   ğŸ“ˆ ../reports/roc_curve.png\")\n",
        "print(f\"   ğŸ“„ ../reports/roc_auc_results.json\")\n",
        "\n",
        "print(f\"\\nğŸ¯ RECOMENDACIONES:\")\n",
        "if test_results['test_f1'] >= 0.8:\n",
        "    print(\"   âœ… Modelo con excelente rendimiento, listo para producciÃ³n\")\n",
        "elif test_results['test_f1'] >= 0.7:\n",
        "    print(\"   âœ… Modelo con buen rendimiento, considerar mejoras adicionales\")\n",
        "elif test_results['test_f1'] >= 0.6:\n",
        "    print(\"   âš ï¸ Modelo con rendimiento aceptable, recomendado optimizar\")\n",
        "else:\n",
        "    print(\"   âŒ Modelo con rendimiento pobre, requiere mejoras significativas\")\n",
        "\n",
        "if imbalance_ratio > 2:\n",
        "    print(\"   ğŸ“Š Considerar tÃ©cnicas de balanceo de clases (SMOTE, undersampling)\")\n",
        "\n",
        "if worst_class_idx != best_class_idx:\n",
        "    print(f\"   ğŸ¯ Enfocar mejoras en la predicciÃ³n de la clase '{worst_class}'\")\n",
        "\n",
        "print(f\"\\nâœ… ANÃLISIS DE MÃ‰TRICAS COMPLETADO\")\n",
        "print(f\"ğŸ¯ El modelo {best_model_name} estÃ¡ listo para implementaciÃ³n\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

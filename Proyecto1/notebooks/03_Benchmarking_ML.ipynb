{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Proyecto 1: Benchmarking de Modelos de Machine Learning\n",
        "## Comparaci√≥n y Evaluaci√≥n de M√∫ltiples Algoritmos\n",
        "\n",
        "**Objetivo**: Entrenar y evaluar m√∫ltiples modelos de machine learning utilizando validaci√≥n cruzada para seleccionar el mejor algoritmo para la predicci√≥n de categor√≠as de venta.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importaci√≥n de bibliotecas para machine learning\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import joblib\n",
        "import warnings\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
        "from sklearn.metrics import (classification_report, confusion_matrix, \n",
        "                           accuracy_score, precision_score, recall_score, \n",
        "                           f1_score, roc_auc_score, roc_curve)\n",
        "import time\n",
        "\n",
        "# Intentar importar XGBoost y LightGBM\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "    XGB_AVAILABLE = True\n",
        "except ImportError:\n",
        "    XGB_AVAILABLE = False\n",
        "    print(\"‚ö†Ô∏è XGBoost no disponible\")\n",
        "\n",
        "try:\n",
        "    import lightgbm as lgb\n",
        "    LGB_AVAILABLE = True\n",
        "except ImportError:\n",
        "    LGB_AVAILABLE = False\n",
        "    print(\"‚ö†Ô∏è LightGBM no disponible\")\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"‚úÖ Bibliotecas de machine learning importadas correctamente\")\n",
        "print(f\"üìä XGBoost disponible: {XGB_AVAILABLE}\")\n",
        "print(f\"üìä LightGBM disponible: {LGB_AVAILABLE}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. Carga de Datos Preprocesados\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cargar los datos preprocesados\n",
        "try:\n",
        "    processed_datasets = joblib.load('../models/processed_datasets.joblib')\n",
        "    label_encoder = joblib.load('../models/label_encoder.joblib')\n",
        "    print(\"‚úÖ Datos preprocesados cargados correctamente\")\n",
        "except FileNotFoundError:\n",
        "    print(\"‚ùå Error: Ejecutar primero el notebook de preprocesamiento\")\n",
        "    print(\"üìã Archivos necesarios:\")\n",
        "    print(\"   ‚Ä¢ ../models/processed_datasets.joblib\")\n",
        "    print(\"   ‚Ä¢ ../models/label_encoder.joblib\")\n",
        "\n",
        "print(f\"\\nüìä DATASETS DISPONIBLES:\")\n",
        "for name in processed_datasets.keys():\n",
        "    dataset = processed_datasets[name]\n",
        "    print(f\"   ‚úÖ {name}:\")\n",
        "    print(f\"      üèãÔ∏è Train: {dataset['X_train'].shape}\")\n",
        "    print(f\"      üß™ Test: {dataset['X_test'].shape}\")\n",
        "\n",
        "print(f\"\\nüéØ CLASES DEL TARGET:\")\n",
        "for i, class_name in enumerate(label_encoder.classes_):\n",
        "    print(f\"   {i}: {class_name}\")\n",
        "\n",
        "# Seleccionar el dataset principal para benchmarking (StandardScaler + OneHot)\n",
        "main_dataset_name = 'StandardScaler_OneHot'\n",
        "main_dataset = processed_datasets[main_dataset_name]\n",
        "\n",
        "X_train = main_dataset['X_train']\n",
        "X_test = main_dataset['X_test']\n",
        "y_train = main_dataset['y_train']\n",
        "y_test = main_dataset['y_test']\n",
        "\n",
        "print(f\"\\nüéØ DATASET PRINCIPAL SELECCIONADO: {main_dataset_name}\")\n",
        "print(f\"   üìä Entrenamiento: {X_train.shape}\")\n",
        "print(f\"   üìä Prueba: {X_test.shape}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. Definici√≥n de Modelos para Benchmarking\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Definir los modelos para benchmarking\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
        "    'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=5),\n",
        "    'Decision Tree': DecisionTreeClassifier(random_state=42, max_depth=10),\n",
        "    'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100),\n",
        "    'Support Vector Machine': SVC(random_state=42, probability=True),\n",
        "    'Naive Bayes': GaussianNB()\n",
        "}\n",
        "\n",
        "# Agregar XGBoost si est√° disponible\n",
        "if XGB_AVAILABLE:\n",
        "    models['XGBoost'] = xgb.XGBClassifier(random_state=42, eval_metric='mlogloss')\n",
        "\n",
        "# Agregar LightGBM si est√° disponible\n",
        "if LGB_AVAILABLE:\n",
        "    models['LightGBM'] = lgb.LGBMClassifier(random_state=42, verbose=-1)\n",
        "\n",
        "print(\"ü§ñ MODELOS DEFINIDOS PARA BENCHMARKING\")\n",
        "print(\"=\"*50)\n",
        "for i, (name, model) in enumerate(models.items(), 1):\n",
        "    print(f\"   {i:2d}. {name}\")\n",
        "    print(f\"       üìã Tipo: {type(model).__name__}\")\n",
        "\n",
        "print(f\"\\nüìä Total de modelos: {len(models)}\")\n",
        "\n",
        "# Configuraci√≥n de validaci√≥n cruzada\n",
        "cv_folds = 5\n",
        "cv_scoring = ['accuracy', 'precision_macro', 'recall_macro', 'f1_macro']\n",
        "\n",
        "print(f\"\\nüîÑ CONFIGURACI√ìN DE VALIDACI√ìN CRUZADA:\")\n",
        "print(f\"   üìä N√∫mero de folds: {cv_folds}\")\n",
        "print(f\"   üìà M√©tricas de evaluaci√≥n: {', '.join(cv_scoring)}\")\n",
        "print(f\"   üéØ Estratificaci√≥n: Activada (mantiene proporci√≥n de clases)\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. Evaluaci√≥n con Validaci√≥n Cruzada\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Realizar validaci√≥n cruzada para todos los modelos\n",
        "cv_results = {}\n",
        "training_times = {}\n",
        "\n",
        "print(\"üîÑ EJECUTANDO VALIDACI√ìN CRUZADA\")\n",
        "print(\"=\"*40)\n",
        "\n",
        "# Configurar validaci√≥n cruzada estratificada\n",
        "skf = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    print(f\"\\nüöÄ Entrenando: {model_name}\")\n",
        "    \n",
        "    # Medir tiempo de entrenamiento\n",
        "    start_time = time.time()\n",
        "    \n",
        "    # Realizar validaci√≥n cruzada para cada m√©trica\n",
        "    cv_results[model_name] = {}\n",
        "    \n",
        "    for metric in cv_scoring:\n",
        "        scores = cross_val_score(model, X_train, y_train, \n",
        "                               cv=skf, scoring=metric, n_jobs=-1)\n",
        "        cv_results[model_name][metric] = {\n",
        "            'scores': scores,\n",
        "            'mean': scores.mean(),\n",
        "            'std': scores.std()\n",
        "        }\n",
        "    \n",
        "    # Guardar tiempo de entrenamiento\n",
        "    end_time = time.time()\n",
        "    training_times[model_name] = end_time - start_time\n",
        "    \n",
        "    print(f\"   ‚è±Ô∏è Tiempo: {training_times[model_name]:.2f}s\")\n",
        "    print(f\"   üìä Accuracy: {cv_results[model_name]['accuracy']['mean']:.4f} (¬±{cv_results[model_name]['accuracy']['std']:.4f})\")\n",
        "    print(f\"   üìä F1-Score: {cv_results[model_name]['f1_macro']['mean']:.4f} (¬±{cv_results[model_name]['f1_macro']['std']:.4f})\")\n",
        "\n",
        "print(f\"\\n‚úÖ VALIDACI√ìN CRUZADA COMPLETADA\")\n",
        "print(f\"üéØ {len(models)} modelos evaluados con {cv_folds}-fold CV\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 4. An√°lisis de Resultados y Comparaci√≥n de Modelos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear tabla resumen de resultados\n",
        "results_df = pd.DataFrame()\n",
        "\n",
        "for model_name in cv_results.keys():\n",
        "    row = {\n",
        "        'Model': model_name,\n",
        "        'Accuracy_Mean': cv_results[model_name]['accuracy']['mean'],\n",
        "        'Accuracy_Std': cv_results[model_name]['accuracy']['std'],\n",
        "        'Precision_Mean': cv_results[model_name]['precision_macro']['mean'],\n",
        "        'Precision_Std': cv_results[model_name]['precision_macro']['std'],\n",
        "        'Recall_Mean': cv_results[model_name]['recall_macro']['mean'],\n",
        "        'Recall_Std': cv_results[model_name]['recall_macro']['std'],\n",
        "        'F1_Mean': cv_results[model_name]['f1_macro']['mean'],\n",
        "        'F1_Std': cv_results[model_name]['f1_macro']['std'],\n",
        "        'Training_Time': training_times[model_name]\n",
        "    }\n",
        "    results_df = pd.concat([results_df, pd.DataFrame([row])], ignore_index=True)\n",
        "\n",
        "# Ordenar por F1-Score promedio\n",
        "results_df = results_df.sort_values('F1_Mean', ascending=False).reset_index(drop=True)\n",
        "\n",
        "print(\"üìä RESULTADOS DE VALIDACI√ìN CRUZADA\")\n",
        "print(\"=\"*80)\n",
        "print(f\"{'Rank':<4} {'Model':<20} {'Accuracy':<12} {'Precision':<12} {'Recall':<12} {'F1-Score':<12} {'Time(s)':<8}\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "for i, row in results_df.iterrows():\n",
        "    print(f\"{i+1:<4} {row['Model']:<20} \"\n",
        "          f\"{row['Accuracy_Mean']:.3f}¬±{row['Accuracy_Std']:.3f} \"\n",
        "          f\"{row['Precision_Mean']:.3f}¬±{row['Precision_Std']:.3f} \"\n",
        "          f\"{row['Recall_Mean']:.3f}¬±{row['Recall_Std']:.3f} \"\n",
        "          f\"{row['F1_Mean']:.3f}¬±{row['F1_Std']:.3f} \"\n",
        "          f\"{row['Training_Time']:<8.2f}\")\n",
        "\n",
        "# Identificar el mejor modelo\n",
        "best_model_name = results_df.iloc[0]['Model']\n",
        "best_f1_score = results_df.iloc[0]['F1_Mean']\n",
        "\n",
        "print(f\"\\nüèÜ MEJOR MODELO: {best_model_name}\")\n",
        "print(f\"   üìä F1-Score: {best_f1_score:.4f}\")\n",
        "print(f\"   ‚è±Ô∏è Tiempo de entrenamiento: {results_df.iloc[0]['Training_Time']:.2f}s\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizaci√≥n de resultados\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "fig.suptitle('üìä COMPARACI√ìN DE MODELOS DE MACHINE LEARNING\\nüéØ Resultados de Validaci√≥n Cruzada', \n",
        "             fontsize=16, fontweight='bold', y=0.98)\n",
        "\n",
        "# Subplot 1: Accuracy comparison\n",
        "ax1 = axes[0,0]\n",
        "accuracy_means = [cv_results[model]['accuracy']['mean'] for model in results_df['Model']]\n",
        "accuracy_stds = [cv_results[model]['accuracy']['std'] for model in results_df['Model']]\n",
        "\n",
        "bars1 = ax1.barh(results_df['Model'], accuracy_means, xerr=accuracy_stds, \n",
        "                 color='skyblue', alpha=0.8, capsize=5)\n",
        "ax1.set_title('üéØ Accuracy Comparison', fontweight='bold', fontsize=12)\n",
        "ax1.set_xlabel('Accuracy Score')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# A√±adir valores en las barras\n",
        "for i, (bar, mean_val) in enumerate(zip(bars1, accuracy_means)):\n",
        "    ax1.text(mean_val + 0.01, bar.get_y() + bar.get_height()/2, \n",
        "             f'{mean_val:.3f}', va='center', fontweight='bold')\n",
        "\n",
        "# Subplot 2: F1-Score comparison\n",
        "ax2 = axes[0,1]\n",
        "f1_means = [cv_results[model]['f1_macro']['mean'] for model in results_df['Model']]\n",
        "f1_stds = [cv_results[model]['f1_macro']['std'] for model in results_df['Model']]\n",
        "\n",
        "bars2 = ax2.barh(results_df['Model'], f1_means, xerr=f1_stds, \n",
        "                 color='lightgreen', alpha=0.8, capsize=5)\n",
        "ax2.set_title('üìà F1-Score Comparison', fontweight='bold', fontsize=12)\n",
        "ax2.set_xlabel('F1-Score (Macro)')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# A√±adir valores en las barras\n",
        "for i, (bar, mean_val) in enumerate(zip(bars2, f1_means)):\n",
        "    ax2.text(mean_val + 0.01, bar.get_y() + bar.get_height()/2, \n",
        "             f'{mean_val:.3f}', va='center', fontweight='bold')\n",
        "\n",
        "# Subplot 3: Training time comparison\n",
        "ax3 = axes[1,0]\n",
        "times = [training_times[model] for model in results_df['Model']]\n",
        "\n",
        "bars3 = ax3.barh(results_df['Model'], times, color='lightcoral', alpha=0.8)\n",
        "ax3.set_title('‚è±Ô∏è Training Time Comparison', fontweight='bold', fontsize=12)\n",
        "ax3.set_xlabel('Training Time (seconds)')\n",
        "ax3.grid(True, alpha=0.3)\n",
        "\n",
        "# A√±adir valores en las barras\n",
        "for bar, time_val in zip(bars3, times):\n",
        "    ax3.text(time_val + max(times)*0.01, bar.get_y() + bar.get_height()/2, \n",
        "             f'{time_val:.2f}s', va='center', fontweight='bold')\n",
        "\n",
        "# Subplot 4: Radar chart de m√©tricas del mejor modelo\n",
        "ax4 = axes[1,1]\n",
        "\n",
        "# Preparar datos para radar chart\n",
        "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
        "best_model_scores = [\n",
        "    cv_results[best_model_name]['accuracy']['mean'],\n",
        "    cv_results[best_model_name]['precision_macro']['mean'],\n",
        "    cv_results[best_model_name]['recall_macro']['mean'],\n",
        "    cv_results[best_model_name]['f1_macro']['mean']\n",
        "]\n",
        "\n",
        "# Crear gr√°fico polar\n",
        "angles = np.linspace(0, 2 * np.pi, len(metrics), endpoint=False).tolist()\n",
        "scores = best_model_scores + [best_model_scores[0]]  # Cerrar el pol√≠gono\n",
        "angles += angles[:1]\n",
        "\n",
        "ax4 = plt.subplot(2, 2, 4, projection='polar')\n",
        "ax4.plot(angles, scores, 'o-', linewidth=2, color='red', alpha=0.8)\n",
        "ax4.fill(angles, scores, alpha=0.25, color='red')\n",
        "ax4.set_xticks(angles[:-1])\n",
        "ax4.set_xticklabels(metrics)\n",
        "ax4.set_ylim(0, 1)\n",
        "ax4.set_title(f'üèÜ {best_model_name}\\\\nM√©tricas de Rendimiento', \n",
        "              fontweight='bold', fontsize=12, pad=20)\n",
        "\n",
        "# A√±adir valores en el radar\n",
        "for angle, score, metric in zip(angles[:-1], best_model_scores, metrics):\n",
        "    ax4.text(angle, score + 0.05, f'{score:.3f}', \n",
        "             ha='center', va='center', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Guardar tabla de resultados\n",
        "results_df.to_csv('../reports/cv_results_comparison.csv', index=False)\n",
        "print(f\"\\nüíæ Resultados guardados: ../reports/cv_results_comparison.csv\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 5. Evaluaci√≥n Final del Mejor Modelo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Entrenar el mejor modelo en todo el conjunto de entrenamiento\n",
        "best_model = models[best_model_name]\n",
        "best_model.fit(X_train, y_train)\n",
        "\n",
        "# Hacer predicciones en el conjunto de prueba\n",
        "y_pred = best_model.predict(X_test)\n",
        "y_pred_proba = best_model.predict_proba(X_test)\n",
        "\n",
        "print(f\"üèÜ EVALUACI√ìN FINAL DEL MEJOR MODELO: {best_model_name}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# M√©tricas en el conjunto de prueba\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "test_precision = precision_score(y_test, y_pred, average='macro')\n",
        "test_recall = recall_score(y_test, y_pred, average='macro')\n",
        "test_f1 = f1_score(y_test, y_pred, average='macro')\n",
        "\n",
        "print(f\"üìä M√âTRICAS EN CONJUNTO DE PRUEBA:\")\n",
        "print(f\"   üéØ Accuracy: {test_accuracy:.4f}\")\n",
        "print(f\"   üìà Precision (macro): {test_precision:.4f}\")\n",
        "print(f\"   üìà Recall (macro): {test_recall:.4f}\")\n",
        "print(f\"   üìà F1-Score (macro): {test_f1:.4f}\")\n",
        "\n",
        "# Comparar con resultados de validaci√≥n cruzada\n",
        "cv_accuracy = cv_results[best_model_name]['accuracy']['mean']\n",
        "cv_f1 = cv_results[best_model_name]['f1_macro']['mean']\n",
        "\n",
        "print(f\"\\nüîÑ COMPARACI√ìN CON VALIDACI√ìN CRUZADA:\")\n",
        "print(f\"   üìä Accuracy - CV: {cv_accuracy:.4f} | Test: {test_accuracy:.4f} | Diff: {abs(cv_accuracy-test_accuracy):.4f}\")\n",
        "print(f\"   üìä F1-Score - CV: {cv_f1:.4f} | Test: {test_f1:.4f} | Diff: {abs(cv_f1-test_f1):.4f}\")\n",
        "\n",
        "# Guardar el modelo entrenado\n",
        "model_filename = f'../models/best_model_{best_model_name.replace(\" \", \"_\")}.joblib'\n",
        "joblib.dump(best_model, model_filename)\n",
        "print(f\"\\nüíæ Mejor modelo guardado: {model_filename}\")\n",
        "\n",
        "# Preparar datos para visualizaci√≥n\n",
        "test_results = {\n",
        "    'model_name': best_model_name,\n",
        "    'test_accuracy': test_accuracy,\n",
        "    'test_precision': test_precision,\n",
        "    'test_recall': test_recall,\n",
        "    'test_f1': test_f1,\n",
        "    'cv_accuracy': cv_accuracy,\n",
        "    'cv_f1': cv_f1\n",
        "}\n",
        "\n",
        "# Guardar m√©tricas\n",
        "joblib.dump(test_results, '../models/test_results.joblib')\n",
        "print(f\"üíæ M√©tricas de prueba guardadas: ../models/test_results.joblib\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

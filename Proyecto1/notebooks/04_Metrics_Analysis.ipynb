{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Proyecto 1: An√°lisis Detallado de M√©tricas\n",
        "## Evaluaci√≥n Completa del Modelo Final\n",
        "\n",
        "**Objetivo**: Generar un an√°lisis completo de m√©tricas incluyendo matriz de confusi√≥n, curva ROC, AUC y reportes de clasificaci√≥n detallados para el mejor modelo seleccionado.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importaci√≥n de bibliotecas para an√°lisis de m√©tricas\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import joblib\n",
        "from sklearn.metrics import (classification_report, confusion_matrix, \n",
        "                           roc_curve, auc, roc_auc_score,\n",
        "                           precision_recall_curve, average_precision_score)\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from itertools import cycle\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configuraci√≥n de visualizaci√≥n\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"‚úÖ Bibliotecas para an√°lisis de m√©tricas importadas correctamente\")\n",
        "print(\"üìä Configuraci√≥n de visualizaci√≥n establecida\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. Carga del Modelo y Datos de Prueba\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cargar modelo y datos necesarios\n",
        "try:\n",
        "    # Cargar datos preprocesados\n",
        "    processed_datasets = joblib.load('../models/processed_datasets.joblib')\n",
        "    label_encoder = joblib.load('../models/label_encoder.joblib')\n",
        "    test_results = joblib.load('../models/test_results.joblib')\n",
        "    \n",
        "    # Obtener el nombre del mejor modelo\n",
        "    best_model_name = test_results['model_name']\n",
        "    model_filename = f'../models/best_model_{best_model_name.replace(\" \", \"_\")}.joblib'\n",
        "    best_model = joblib.load(model_filename)\n",
        "    \n",
        "    print(\"‚úÖ Modelo y datos cargados correctamente\")\n",
        "    print(f\"üèÜ Mejor modelo: {best_model_name}\")\n",
        "    \n",
        "except FileNotFoundError as e:\n",
        "    print(f\"‚ùå Error: {e}\")\n",
        "    print(\"üìã Aseg√∫rate de ejecutar primero los notebooks de preprocesamiento y benchmarking\")\n",
        "\n",
        "# Obtener datos de prueba del dataset principal\n",
        "main_dataset = processed_datasets['StandardScaler_OneHot']\n",
        "X_test = main_dataset['X_test']\n",
        "y_test = main_dataset['y_test']\n",
        "\n",
        "# Hacer predicciones\n",
        "y_pred = best_model.predict(X_test)\n",
        "y_pred_proba = best_model.predict_proba(X_test)\n",
        "\n",
        "print(f\"\\nüìä INFORMACI√ìN DEL CONJUNTO DE PRUEBA:\")\n",
        "print(f\"   üß™ Tama√±o: {X_test.shape[0]} muestras\")\n",
        "print(f\"   üìã Features: {X_test.shape[1]}\")\n",
        "print(f\"   üéØ Clases: {len(label_encoder.classes_)}\")\n",
        "\n",
        "print(f\"\\nüîÆ PREDICCIONES GENERADAS:\")\n",
        "print(f\"   ‚úÖ Predicciones discretas: {y_pred.shape}\")\n",
        "print(f\"   üìä Probabilidades: {y_pred_proba.shape}\")\n",
        "\n",
        "# Mostrar distribuci√≥n de predicciones\n",
        "pred_distribution = pd.Series(y_pred).value_counts().sort_index()\n",
        "print(f\"\\nüìä DISTRIBUCI√ìN DE PREDICCIONES:\")\n",
        "for class_idx, count in pred_distribution.items():\n",
        "    class_name = label_encoder.inverse_transform([class_idx])[0]\n",
        "    percentage = (count / len(y_pred)) * 100\n",
        "    print(f\"   {class_name}: {count} ({percentage:.1f}%)\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. Reporte de Clasificaci√≥n Detallado\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generar reporte de clasificaci√≥n detallado\n",
        "class_names = label_encoder.classes_\n",
        "\n",
        "print(\"üìã REPORTE DE CLASIFICACI√ìN DETALLADO\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Reporte de clasificaci√≥n con nombres de clases\n",
        "classification_rep = classification_report(\n",
        "    y_test, y_pred, \n",
        "    target_names=class_names,\n",
        "    output_dict=True\n",
        ")\n",
        "\n",
        "# Mostrar reporte formateado\n",
        "print(classification_report(y_test, y_pred, target_names=class_names))\n",
        "\n",
        "# Crear DataFrame para mejor an√°lisis\n",
        "report_df = pd.DataFrame(classification_rep).transpose()\n",
        "report_df = report_df.round(4)\n",
        "\n",
        "print(\"\\nüìä M√âTRICAS POR CLASE:\")\n",
        "print(\"=\"*50)\n",
        "for class_name in class_names:\n",
        "    if class_name in report_df.index:\n",
        "        precision = report_df.loc[class_name, 'precision']\n",
        "        recall = report_df.loc[class_name, 'recall']\n",
        "        f1 = report_df.loc[class_name, 'f1-score']\n",
        "        support = int(report_df.loc[class_name, 'support'])\n",
        "        \n",
        "        print(f\"\\nüè∑Ô∏è Clase: {class_name}\")\n",
        "        print(f\"   üìà Precision: {precision:.4f}\")\n",
        "        print(f\"   üìà Recall: {recall:.4f}\")\n",
        "        print(f\"   üìà F1-Score: {f1:.4f}\")\n",
        "        print(f\"   üìä Support: {support} muestras\")\n",
        "\n",
        "# M√©tricas globales\n",
        "macro_avg = report_df.loc['macro avg']\n",
        "weighted_avg = report_df.loc['weighted avg']\n",
        "\n",
        "print(f\"\\nüåê M√âTRICAS GLOBALES:\")\n",
        "print(f\"   üìä Macro Average:\")\n",
        "print(f\"      ‚Ä¢ Precision: {macro_avg['precision']:.4f}\")\n",
        "print(f\"      ‚Ä¢ Recall: {macro_avg['recall']:.4f}\")\n",
        "print(f\"      ‚Ä¢ F1-Score: {macro_avg['f1-score']:.4f}\")\n",
        "print(f\"   üìä Weighted Average:\")\n",
        "print(f\"      ‚Ä¢ Precision: {weighted_avg['precision']:.4f}\")\n",
        "print(f\"      ‚Ä¢ Recall: {weighted_avg['recall']:.4f}\")\n",
        "print(f\"      ‚Ä¢ F1-Score: {weighted_avg['f1-score']:.4f}\")\n",
        "\n",
        "# Guardar reporte de clasificaci√≥n\n",
        "report_df.to_csv('../reports/classification_report.csv')\n",
        "print(f\"\\nüíæ Reporte guardado: ../reports/classification_report.csv\")\n",
        "\n",
        "# Guardar reporte de clasificaci√≥n en texto\n",
        "with open('../reports/classification_report.txt', 'w') as f:\n",
        "    f.write(f\"Reporte de Clasificaci√≥n - Modelo: {best_model_name}\\n\")\n",
        "    f.write(\"=\"*60 + \"\\n\\n\")\n",
        "    f.write(classification_report(y_test, y_pred, target_names=class_names))\n",
        "\n",
        "print(f\"üíæ Reporte en texto guardado: ../reports/classification_report.txt\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. Matriz de Confusi√≥n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear y visualizar la matriz de confusi√≥n\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Crear visualizaci√≥n de la matriz de confusi√≥n\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "fig.suptitle(f'üìä MATRIZ DE CONFUSI√ìN - {best_model_name}\\nüéØ An√°lisis de Errores de Clasificaci√≥n', \n",
        "             fontsize=16, fontweight='bold', y=1.02)\n",
        "\n",
        "# Matriz de confusi√≥n en n√∫meros absolutos\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=class_names, yticklabels=class_names,\n",
        "            ax=axes[0], cbar_kws={'shrink': 0.8})\n",
        "axes[0].set_title('üìà Valores Absolutos', fontweight='bold', fontsize=14)\n",
        "axes[0].set_xlabel('Predicci√≥n')\n",
        "axes[0].set_ylabel('Valor Real')\n",
        "\n",
        "# Matriz de confusi√≥n normalizada (porcentajes)\n",
        "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Oranges',\n",
        "            xticklabels=class_names, yticklabels=class_names,\n",
        "            ax=axes[1], cbar_kws={'shrink': 0.8})\n",
        "axes[1].set_title('üìä Porcentajes (Normalizado)', fontweight='bold', fontsize=14)\n",
        "axes[1].set_xlabel('Predicci√≥n')\n",
        "axes[1].set_ylabel('Valor Real')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('../reports/confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# An√°lisis detallado de la matriz de confusi√≥n\n",
        "print(\"üìä AN√ÅLISIS DETALLADO DE LA MATRIZ DE CONFUSI√ìN\")\n",
        "print(\"=\"*55)\n",
        "\n",
        "total_samples = cm.sum()\n",
        "correct_predictions = np.trace(cm)\n",
        "accuracy = correct_predictions / total_samples\n",
        "\n",
        "print(f\"üéØ ESTAD√çSTICAS GENERALES:\")\n",
        "print(f\"   üìä Total de muestras: {total_samples}\")\n",
        "print(f\"   ‚úÖ Predicciones correctas: {correct_predictions}\")\n",
        "print(f\"   ‚ùå Predicciones incorrectas: {total_samples - correct_predictions}\")\n",
        "print(f\"   üìà Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
        "\n",
        "print(f\"\\nüîç AN√ÅLISIS POR CLASE:\")\n",
        "for i, class_name in enumerate(class_names):\n",
        "    # Verdaderos positivos, falsos positivos, falsos negativos\n",
        "    tp = cm[i, i]\n",
        "    fp = cm[:, i].sum() - tp\n",
        "    fn = cm[i, :].sum() - tp\n",
        "    tn = total_samples - tp - fp - fn\n",
        "    \n",
        "    # M√©tricas por clase\n",
        "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "    \n",
        "    print(f\"\\nüè∑Ô∏è Clase: {class_name}\")\n",
        "    print(f\"   ‚úÖ Verdaderos Positivos (TP): {tp}\")\n",
        "    print(f\"   ‚ùå Falsos Positivos (FP): {fp}\")\n",
        "    print(f\"   ‚ùå Falsos Negativos (FN): {fn}\")\n",
        "    print(f\"   ‚úÖ Verdaderos Negativos (TN): {tn}\")\n",
        "    print(f\"   üìà Precision: {precision:.4f}\")\n",
        "    print(f\"   üìà Recall (Sensibilidad): {recall:.4f}\")\n",
        "    print(f\"   üìà Especificidad: {specificity:.4f}\")\n",
        "\n",
        "# Identificar errores m√°s comunes\n",
        "print(f\"\\n‚ùå ERRORES M√ÅS COMUNES:\")\n",
        "print(\"=\"*30)\n",
        "\n",
        "# Encontrar las confusiones m√°s frecuentes (excluyendo la diagonal)\n",
        "errors = []\n",
        "for i in range(len(class_names)):\n",
        "    for j in range(len(class_names)):\n",
        "        if i != j and cm[i, j] > 0:\n",
        "            errors.append((class_names[i], class_names[j], cm[i, j]))\n",
        "\n",
        "# Ordenar por frecuencia de error\n",
        "errors.sort(key=lambda x: x[2], reverse=True)\n",
        "\n",
        "for true_class, pred_class, count in errors[:5]:  # Top 5 errores\n",
        "    percentage = (count / total_samples) * 100\n",
        "    print(f\"   üîÑ {true_class} ‚Üí {pred_class}: {count} veces ({percentage:.2f}%)\")\n",
        "\n",
        "# Guardar matriz de confusi√≥n\n",
        "np.savetxt('../reports/confusion_matrix.csv', cm, delimiter=',', fmt='%d')\n",
        "print(f\"\\nüíæ Matriz de confusi√≥n guardada: ../reports/confusion_matrix.csv\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 4. Curvas ROC y AUC\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calcular curvas ROC y AUC para clasificaci√≥n multiclase\n",
        "n_classes = len(class_names)\n",
        "\n",
        "# Binarizar las etiquetas para ROC multiclase\n",
        "y_test_binarized = label_binarize(y_test, classes=range(n_classes))\n",
        "\n",
        "# Si solo hay 2 clases, label_binarize devuelve un array 1D\n",
        "if n_classes == 2:\n",
        "    y_test_binarized = np.column_stack([1 - y_test_binarized, y_test_binarized])\n",
        "\n",
        "print(\"üìà AN√ÅLISIS DE CURVAS ROC Y AUC\")\n",
        "print(\"=\"*40)\n",
        "\n",
        "# Calcular ROC para cada clase\n",
        "fpr = dict()\n",
        "tpr = dict()\n",
        "roc_auc = dict()\n",
        "\n",
        "for i in range(n_classes):\n",
        "    fpr[i], tpr[i], _ = roc_curve(y_test_binarized[:, i], y_pred_proba[:, i])\n",
        "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "# Calcular micro-average ROC\n",
        "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test_binarized.ravel(), y_pred_proba.ravel())\n",
        "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
        "\n",
        "# Calcular macro-average ROC\n",
        "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
        "mean_tpr = np.zeros_like(all_fpr)\n",
        "for i in range(n_classes):\n",
        "    mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
        "mean_tpr /= n_classes\n",
        "\n",
        "fpr[\"macro\"] = all_fpr\n",
        "tpr[\"macro\"] = mean_tpr\n",
        "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
        "\n",
        "# Visualizar curvas ROC\n",
        "plt.figure(figsize=(14, 10))\n",
        "\n",
        "# Colores para cada clase\n",
        "colors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'red', 'green', 'purple'])\n",
        "\n",
        "# Plotear ROC para cada clase\n",
        "for i, color in zip(range(n_classes), colors):\n",
        "    plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
        "             label=f'ROC {class_names[i]} (AUC = {roc_auc[i]:.3f})')\n",
        "\n",
        "# Plotear micro y macro average\n",
        "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
        "         label=f'Micro-average ROC (AUC = {roc_auc[\"micro\"]:.3f})',\n",
        "         color='deeppink', linestyle=':', linewidth=3)\n",
        "\n",
        "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
        "         label=f'Macro-average ROC (AUC = {roc_auc[\"macro\"]:.3f})',\n",
        "         color='navy', linestyle=':', linewidth=3)\n",
        "\n",
        "# L√≠nea de referencia (clasificador aleatorio)\n",
        "plt.plot([0, 1], [0, 1], 'k--', lw=2, label='Clasificador Aleatorio (AUC = 0.500)')\n",
        "\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('Tasa de Falsos Positivos (1 - Especificidad)', fontsize=12, fontweight='bold')\n",
        "plt.ylabel('Tasa de Verdaderos Positivos (Sensibilidad)', fontsize=12, fontweight='bold')\n",
        "plt.title(f'üìà CURVAS ROC MULTICLASE - {best_model_name}\\\\nüéØ Receiver Operating Characteristic', \n",
        "          fontsize=16, fontweight='bold', pad=20)\n",
        "plt.legend(loc=\"lower right\", fontsize=10)\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# A√±adir anotaciones\n",
        "plt.text(0.6, 0.2, f'Mejor Rendimiento:\\\\nMacro AUC = {roc_auc[\"macro\"]:.3f}\\\\nMicro AUC = {roc_auc[\"micro\"]:.3f}',\n",
        "         bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\", alpha=0.8),\n",
        "         fontsize=11, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('../reports/roc_curve.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Mostrar AUC por clase\n",
        "print(f\"üìä AUC POR CLASE:\")\n",
        "for i, class_name in enumerate(class_names):\n",
        "    print(f\"   üè∑Ô∏è {class_name}: {roc_auc[i]:.4f}\")\n",
        "\n",
        "print(f\"\\nüìä AUC PROMEDIO:\")\n",
        "print(f\"   üîç Micro-average: {roc_auc['micro']:.4f}\")\n",
        "print(f\"   üîç Macro-average: {roc_auc['macro']:.4f}\")\n",
        "\n",
        "# Interpretaci√≥n de AUC\n",
        "macro_auc = roc_auc['macro']\n",
        "if macro_auc >= 0.9:\n",
        "    interpretation = \"Excelente\"\n",
        "elif macro_auc >= 0.8:\n",
        "    interpretation = \"Bueno\"\n",
        "elif macro_auc >= 0.7:\n",
        "    interpretation = \"Aceptable\"\n",
        "elif macro_auc >= 0.6:\n",
        "    interpretation = \"Pobre\"\n",
        "else:\n",
        "    interpretation = \"Muy Pobre\"\n",
        "\n",
        "print(f\"\\nüí° INTERPRETACI√ìN DEL RENDIMIENTO:\")\n",
        "print(f\"   üìà AUC Macro: {macro_auc:.4f} ‚Üí {interpretation}\")\n",
        "\n",
        "# Guardar resultados ROC\n",
        "roc_results = {\n",
        "    'class_auc': {class_names[i]: roc_auc[i] for i in range(n_classes)},\n",
        "    'micro_auc': roc_auc['micro'],\n",
        "    'macro_auc': roc_auc['macro']\n",
        "}\n",
        "\n",
        "import json\n",
        "with open('../reports/roc_auc_results.json', 'w') as f:\n",
        "    json.dump(roc_results, f, indent=2)\n",
        "\n",
        "print(f\"\\nüíæ Resultados ROC guardados: ../reports/roc_auc_results.json\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 5. Resumen Final y Conclusiones\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generar resumen final completo\n",
        "print(\"üéØ RESUMEN FINAL DEL AN√ÅLISIS DE M√âTRICAS\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "print(f\"\\nüèÜ MODELO SELECCIONADO: {best_model_name}\")\n",
        "print(f\"üìä Dataset: Clasificaci√≥n de categor√≠as de venta (retail)\")\n",
        "print(f\"üéØ Problema: Clasificaci√≥n multiclase ({n_classes} clases)\")\n",
        "\n",
        "print(f\"\\nüìà RENDIMIENTO EN CONJUNTO DE PRUEBA:\")\n",
        "print(f\"   üéØ Accuracy: {test_results['test_accuracy']:.4f}\")\n",
        "print(f\"   üìä Precision (macro): {test_results['test_precision']:.4f}\")\n",
        "print(f\"   üìä Recall (macro): {test_results['test_recall']:.4f}\")\n",
        "print(f\"   üìä F1-Score (macro): {test_results['test_f1']:.4f}\")\n",
        "print(f\"   üìà AUC (macro): {roc_auc['macro']:.4f}\")\n",
        "\n",
        "print(f\"\\nüîç AN√ÅLISIS POR CLASE:\")\n",
        "for i, class_name in enumerate(class_names):\n",
        "    if class_name in report_df.index:\n",
        "        precision = report_df.loc[class_name, 'precision']\n",
        "        recall = report_df.loc[class_name, 'recall']\n",
        "        f1 = report_df.loc[class_name, 'f1-score']\n",
        "        auc_score = roc_auc[i]\n",
        "        support = int(report_df.loc[class_name, 'support'])\n",
        "        \n",
        "        print(f\"   üè∑Ô∏è {class_name}:\")\n",
        "        print(f\"      ‚Ä¢ Precision: {precision:.3f} | Recall: {recall:.3f} | F1: {f1:.3f} | AUC: {auc_score:.3f}\")\n",
        "        print(f\"      ‚Ä¢ Muestras: {support}\")\n",
        "\n",
        "print(f\"\\nüí° INSIGHTS CLAVE:\")\n",
        "# Identificar la mejor y peor clase\n",
        "best_class_idx = np.argmax([roc_auc[i] for i in range(n_classes)])\n",
        "worst_class_idx = np.argmin([roc_auc[i] for i in range(n_classes)])\n",
        "\n",
        "best_class = class_names[best_class_idx]\n",
        "worst_class = class_names[worst_class_idx]\n",
        "\n",
        "print(f\"   üèÜ Mejor clase predicha: {best_class} (AUC: {roc_auc[best_class_idx]:.3f})\")\n",
        "print(f\"   üìâ Clase m√°s dif√≠cil: {worst_class} (AUC: {roc_auc[worst_class_idx]:.3f})\")\n",
        "\n",
        "# An√°lisis de balance del dataset\n",
        "class_distribution = pd.Series(y_test).value_counts().sort_index()\n",
        "most_frequent_class = label_encoder.inverse_transform([class_distribution.idxmax()])[0]\n",
        "least_frequent_class = label_encoder.inverse_transform([class_distribution.idxmin()])[0]\n",
        "\n",
        "print(f\"   üìä Clase m√°s frecuente: {most_frequent_class} ({class_distribution.max()} muestras)\")\n",
        "print(f\"   üìä Clase menos frecuente: {least_frequent_class} ({class_distribution.min()} muestras)\")\n",
        "\n",
        "# Evaluar si hay desbalance\n",
        "max_count = class_distribution.max()\n",
        "min_count = class_distribution.min()\n",
        "imbalance_ratio = max_count / min_count\n",
        "\n",
        "if imbalance_ratio > 2:\n",
        "    print(f\"   ‚ö†Ô∏è Dataset desbalanceado (ratio: {imbalance_ratio:.1f}:1)\")\n",
        "else:\n",
        "    print(f\"   ‚úÖ Dataset relativamente balanceado (ratio: {imbalance_ratio:.1f}:1)\")\n",
        "\n",
        "print(f\"\\nüìã ARCHIVOS GENERADOS:\")\n",
        "print(f\"   üìÑ ../reports/classification_report.csv\")\n",
        "print(f\"   üìÑ ../reports/classification_report.txt\")\n",
        "print(f\"   üìä ../reports/confusion_matrix.png\")\n",
        "print(f\"   üìä ../reports/confusion_matrix.csv\")\n",
        "print(f\"   üìà ../reports/roc_curve.png\")\n",
        "print(f\"   üìÑ ../reports/roc_auc_results.json\")\n",
        "\n",
        "print(f\"\\nüéØ RECOMENDACIONES:\")\n",
        "if test_results['test_f1'] >= 0.8:\n",
        "    print(\"   ‚úÖ Modelo con excelente rendimiento, listo para producci√≥n\")\n",
        "elif test_results['test_f1'] >= 0.7:\n",
        "    print(\"   ‚úÖ Modelo con buen rendimiento, considerar mejoras adicionales\")\n",
        "elif test_results['test_f1'] >= 0.6:\n",
        "    print(\"   ‚ö†Ô∏è Modelo con rendimiento aceptable, recomendado optimizar\")\n",
        "else:\n",
        "    print(\"   ‚ùå Modelo con rendimiento pobre, requiere mejoras significativas\")\n",
        "\n",
        "if imbalance_ratio > 2:\n",
        "    print(\"   üìä Considerar t√©cnicas de balanceo de clases (SMOTE, undersampling)\")\n",
        "\n",
        "if worst_class_idx != best_class_idx:\n",
        "    print(f\"   üéØ Enfocar mejoras en la predicci√≥n de la clase '{worst_class}'\")\n",
        "\n",
        "print(f\"\\n‚úÖ AN√ÅLISIS DE M√âTRICAS COMPLETADO\")\n",
        "print(f\"üéØ El modelo {best_model_name} est√° listo para implementaci√≥n\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Proyecto 1: Preprocesamiento de Datos\n",
        "## Preparación de Datos para Machine Learning\n",
        "\n",
        "**Objetivo**: Implementar pipelines de preprocesamiento usando ColumnTransformer y técnicas avanzadas de transformación de datos para preparar el dataset para modelado de machine learning.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importación de bibliotecas para preprocesamiento\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import (StandardScaler, MinMaxScaler, RobustScaler, \n",
        "                                 LabelEncoder, OneHotEncoder, OrdinalEncoder)\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "import joblib\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"✅ Bibliotecas de preprocesamiento importadas correctamente\")\n",
        "print(\"🔧 Listo para implementar pipelines de transformación\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. Carga de Datos Procesados del EDA\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cargar el dataset procesado del EDA\n",
        "try:\n",
        "    df = pd.read_csv('../data_processed_eda.csv')\n",
        "    print(\"✅ Dataset del EDA cargado correctamente\")\n",
        "except FileNotFoundError:\n",
        "    # Cargar dataset original y aplicar transformaciones del EDA\n",
        "    df = pd.read_csv('../retail_sales_dataset.csv')\n",
        "    \n",
        "    # Convertir la columna Date a datetime\n",
        "    df['Date'] = pd.to_datetime(df['Date'])\n",
        "    \n",
        "    # Extraer características temporales\n",
        "    df['Year'] = df['Date'].dt.year\n",
        "    df['Month'] = df['Date'].dt.month\n",
        "    df['Month_Name'] = df['Date'].dt.strftime('%B')\n",
        "    df['Day'] = df['Date'].dt.day\n",
        "    df['DayOfWeek'] = df['Date'].dt.dayofweek\n",
        "    df['DayName'] = df['Date'].dt.strftime('%A')\n",
        "    df['Quarter'] = df['Date'].dt.quarter\n",
        "    \n",
        "    # Clasificación de ventas\n",
        "    def clasificador_ventas(amount):\n",
        "        if amount >= 1000:\n",
        "            return 'Alta'\n",
        "        elif amount >= 300:\n",
        "            return 'Media'\n",
        "        else:\n",
        "            return 'Baja'\n",
        "    \n",
        "    df['Sales_Category'] = df['Total Amount'].apply(clasificador_ventas)\n",
        "    \n",
        "    # Clasificación de edad\n",
        "    def clasificador_edad(age):\n",
        "        if age >= 50:\n",
        "            return 'Adulto Mayor'\n",
        "        elif age >= 30 and age < 50:\n",
        "            return 'Adulto'\n",
        "        else:\n",
        "            return 'Joven'\n",
        "    \n",
        "    df['Age_Group'] = df['Age'].apply(clasificador_edad)\n",
        "    \n",
        "    # Normalización Min-Max\n",
        "    min_sales = df['Total Amount'].min()\n",
        "    max_sales = df['Total Amount'].max()\n",
        "    df['Total_Amount_Normalized'] = (df['Total Amount'] - min_sales) / (max_sales - min_sales)\n",
        "    \n",
        "    print(\"✅ Transformaciones del EDA aplicadas al dataset original\")\n",
        "\n",
        "print(f\"📊 Dimensiones del dataset: {df.shape}\")\n",
        "print(f\"📋 Columnas disponibles: {len(df.columns)}\")\n",
        "print(\"\\n🔍 Primeras filas:\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. Definición de Variables y Target\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Definir diferentes problemas de machine learning que podemos resolver\n",
        "\n",
        "print(\"🎯 DEFINICIÓN DE PROBLEMAS DE MACHINE LEARNING\")\n",
        "print(\"=\"*55)\n",
        "\n",
        "# Problema 1: Clasificación de categorías de venta (Sales_Category)\n",
        "# Target: Sales_Category (Alta, Media, Baja)\n",
        "print(\"\\n📊 Problema 1: Clasificación de Categorías de Venta\")\n",
        "print(\"   🎯 Target: Sales_Category (Alta, Media, Baja)\")\n",
        "print(\"   📈 Tipo: Clasificación multiclase\")\n",
        "\n",
        "# Problema 2: Regresión para predecir Total Amount\n",
        "print(\"\\n💰 Problema 2: Predicción del Monto Total de Venta\")\n",
        "print(\"   🎯 Target: Total Amount (variable continua)\")\n",
        "print(\"   📈 Tipo: Regresión\")\n",
        "\n",
        "# Problema 3: Clasificación binaria de ventas altas\n",
        "# Crear variable binaria para ventas >= 1000\n",
        "df['High_Sales'] = (df['Total Amount'] >= 1000).astype(int)\n",
        "print(\"\\n🚀 Problema 3: Predicción de Ventas Altas (Binary)\")\n",
        "print(\"   🎯 Target: High_Sales (1: >= $1000, 0: < $1000)\")\n",
        "print(\"   📈 Tipo: Clasificación binaria\")\n",
        "\n",
        "# Seleccionar el problema principal: Clasificación de Sales_Category\n",
        "target_variable = 'Sales_Category'\n",
        "print(f\"\\n✅ PROBLEMA SELECCIONADO: Clasificación de {target_variable}\")\n",
        "\n",
        "# Definir features para el modelo\n",
        "# Excluir variables que no deben usarse como features\n",
        "exclude_columns = [\n",
        "    'Transaction ID', 'Customer ID', 'Date',  # IDs y fechas\n",
        "    'Total Amount', 'Total_Amount_Normalized',  # Target leak\n",
        "    'Sales_Category', 'High_Sales'  # Variables target\n",
        "]\n",
        "\n",
        "features_all = [col for col in df.columns if col not in exclude_columns]\n",
        "print(f\"\\n📋 FEATURES DISPONIBLES ({len(features_all)}):\")\n",
        "for i, feature in enumerate(features_all, 1):\n",
        "    print(f\"   {i:2d}. {feature}\")\n",
        "\n",
        "# Separar features por tipo\n",
        "numeric_features = df[features_all].select_dtypes(include=[np.number]).columns.tolist()\n",
        "categorical_features = df[features_all].select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "print(f\"\\n🔢 FEATURES NUMÉRICAS ({len(numeric_features)}):\")\n",
        "for feature in numeric_features:\n",
        "    print(f\"   • {feature}\")\n",
        "\n",
        "print(f\"\\n🏷️ FEATURES CATEGÓRICAS ({len(categorical_features)}):\")\n",
        "for feature in categorical_features:\n",
        "    unique_values = df[feature].nunique()\n",
        "    print(f\"   • {feature} ({unique_values} valores únicos)\")\n",
        "\n",
        "# Verificar distribución del target\n",
        "print(f\"\\n🎯 DISTRIBUCIÓN DE LA VARIABLE TARGET ({target_variable}):\")\n",
        "target_distribution = df[target_variable].value_counts()\n",
        "for category, count in target_distribution.items():\n",
        "    percentage = (count / len(df)) * 100\n",
        "    print(f\"   📊 {category}: {count} ({percentage:.1f}%)\")\"\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. Implementación de Pipelines con ColumnTransformer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear diferentes pipelines de preprocesamiento\n",
        "\n",
        "print(\"🔧 CREACIÓN DE PIPELINES DE PREPROCESAMIENTO\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Pipeline 1: StandardScaler para numéricas + OneHotEncoder para categóricas\n",
        "numeric_pipeline_1 = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_pipeline_1 = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "preprocessor_1 = ColumnTransformer([\n",
        "    ('numeric', numeric_pipeline_1, numeric_features),\n",
        "    ('categorical', categorical_pipeline_1, categorical_features)\n",
        "])\n",
        "\n",
        "print(\"✅ Pipeline 1: StandardScaler + OneHotEncoder\")\n",
        "\n",
        "# Pipeline 2: MinMaxScaler para numéricas + OneHotEncoder para categóricas\n",
        "numeric_pipeline_2 = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', MinMaxScaler())\n",
        "])\n",
        "\n",
        "preprocessor_2 = ColumnTransformer([\n",
        "    ('numeric', numeric_pipeline_2, numeric_features),\n",
        "    ('categorical', categorical_pipeline_1, categorical_features)\n",
        "])\n",
        "\n",
        "print(\"✅ Pipeline 2: MinMaxScaler + OneHotEncoder\")\n",
        "\n",
        "# Pipeline 3: RobustScaler para numéricas + OrdinalEncoder para categóricas\n",
        "categorical_pipeline_3 = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('ordinal', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))\n",
        "])\n",
        "\n",
        "numeric_pipeline_3 = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', RobustScaler())\n",
        "])\n",
        "\n",
        "preprocessor_3 = ColumnTransformer([\n",
        "    ('numeric', numeric_pipeline_3, numeric_features),\n",
        "    ('categorical', categorical_pipeline_3, categorical_features)\n",
        "])\n",
        "\n",
        "print(\"✅ Pipeline 3: RobustScaler + OrdinalEncoder\")\n",
        "\n",
        "# Crear diccionario de preprocessors\n",
        "preprocessors = {\n",
        "    'StandardScaler_OneHot': preprocessor_1,\n",
        "    'MinMaxScaler_OneHot': preprocessor_2,\n",
        "    'RobustScaler_Ordinal': preprocessor_3\n",
        "}\n",
        "\n",
        "print(f\"\\n📊 Total de pipelines creados: {len(preprocessors)}\")\n",
        "print(\"🎯 Cada pipeline maneja automáticamente:\")\n",
        "print(\"   • Imputación de valores faltantes\")\n",
        "print(\"   • Escalado de variables numéricas\")\n",
        "print(\"   • Codificación de variables categóricas\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 4. División de Datos y Aplicación de Transformaciones\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preparar datos para machine learning\n",
        "X = df[features_all]\n",
        "y = df[target_variable]\n",
        "\n",
        "# Codificar la variable target\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "print(\"📊 PREPARACIÓN DE DATOS PARA MACHINE LEARNING\")\n",
        "print(\"=\"*50)\n",
        "print(f\"✅ Features (X): {X.shape}\")\n",
        "print(f\"✅ Target (y): {y.shape}\")\n",
        "print(f\"📋 Clases del target: {list(label_encoder.classes_)}\")\n",
        "print(f\"🔢 Target codificado: {dict(zip(label_encoder.classes_, range(len(label_encoder.classes_))))}\")\n",
        "\n",
        "# División en conjuntos de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y_encoded, \n",
        "    test_size=0.2, \n",
        "    random_state=42, \n",
        "    stratify=y_encoded\n",
        ")\n",
        "\n",
        "print(f\"\\n📊 DIVISIÓN DE DATOS:\")\n",
        "print(f\"   🏋️ Entrenamiento: {X_train.shape[0]} muestras ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
        "print(f\"   🧪 Prueba: {X_test.shape[0]} muestras ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
        "\n",
        "# Verificar distribución de clases en cada conjunto\n",
        "train_distribution = pd.Series(y_train).value_counts().sort_index()\n",
        "test_distribution = pd.Series(y_test).value_counts().sort_index()\n",
        "\n",
        "print(f\"\\n🎯 DISTRIBUCIÓN DE CLASES:\")\n",
        "print(\"Conjunto de Entrenamiento:\")\n",
        "for class_idx, count in train_distribution.items():\n",
        "    class_name = label_encoder.inverse_transform([class_idx])[0]\n",
        "    print(f\"   📊 {class_name}: {count} ({count/len(y_train)*100:.1f}%)\")\n",
        "\n",
        "print(\"Conjunto de Prueba:\")\n",
        "for class_idx, count in test_distribution.items():\n",
        "    class_name = label_encoder.inverse_transform([class_idx])[0]\n",
        "    print(f\"   📊 {class_name}: {count} ({count/len(y_test)*100:.1f}%)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Aplicar cada pipeline de preprocesamiento y guardar resultados\n",
        "processed_datasets = {}\n",
        "\n",
        "print(\"🔧 APLICACIÓN DE PIPELINES DE PREPROCESAMIENTO\")\n",
        "print(\"=\"*55)\n",
        "\n",
        "for name, preprocessor in preprocessors.items():\n",
        "    print(f\"\\n🚀 Aplicando pipeline: {name}\")\n",
        "    \n",
        "    # Ajustar y transformar datos de entrenamiento\n",
        "    X_train_processed = preprocessor.fit_transform(X_train)\n",
        "    \n",
        "    # Transformar datos de prueba (solo transform, no fit)\n",
        "    X_test_processed = preprocessor.transform(X_test)\n",
        "    \n",
        "    # Guardar resultados\n",
        "    processed_datasets[name] = {\n",
        "        'X_train': X_train_processed,\n",
        "        'X_test': X_test_processed,\n",
        "        'y_train': y_train,\n",
        "        'y_test': y_test,\n",
        "        'preprocessor': preprocessor,\n",
        "        'label_encoder': label_encoder\n",
        "    }\n",
        "    \n",
        "    print(f\"   ✅ Entrenamiento: {X_train_processed.shape}\")\n",
        "    print(f\"   ✅ Prueba: {X_test_processed.shape}\")\n",
        "    \n",
        "    # Mostrar información sobre las transformaciones aplicadas\n",
        "    if hasattr(preprocessor, 'transformers_'):\n",
        "        for transformer_name, transformer, columns in preprocessor.transformers_:\n",
        "            if transformer_name != 'remainder':\n",
        "                print(f\"   🔹 {transformer_name}: {len(columns)} columnas\")\n",
        "\n",
        "print(f\"\\n📊 RESUMEN DE DATASETS PROCESADOS:\")\n",
        "for name in processed_datasets.keys():\n",
        "    print(f\"   ✅ {name}: Listo para machine learning\")\n",
        "\n",
        "# Guardar los preprocessors para uso futuro\n",
        "import os\n",
        "os.makedirs('../models', exist_ok=True)\n",
        "\n",
        "for name, preprocessor in preprocessors.items():\n",
        "    filename = f'../models/preprocessor_{name}.joblib'\n",
        "    joblib.dump(preprocessor, filename)\n",
        "    print(f\"💾 Preprocessor guardado: {filename}\")\n",
        "\n",
        "# Guardar label encoder\n",
        "joblib.dump(label_encoder, '../models/label_encoder.joblib')\n",
        "print(f\"💾 Label encoder guardado: ../models/label_encoder.joblib\")\n",
        "\n",
        "print(f\"\\n✅ PREPROCESAMIENTO COMPLETADO\")\n",
        "print(f\"🎯 {len(processed_datasets)} datasets listos para benchmarking de modelos\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 5. Análisis de las Transformaciones Aplicadas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analizar el impacto de diferentes transformaciones\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"📊 ANÁLISIS DEL IMPACTO DE LAS TRANSFORMACIONES\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Seleccionar una variable numérica para comparar transformaciones\n",
        "sample_feature = 'Age'  # Usar Age como ejemplo\n",
        "original_data = X_train[sample_feature]\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "fig.suptitle(f'📊 COMPARACIÓN DE TRANSFORMACIONES - Variable: {sample_feature}', \n",
        "             fontsize=16, fontweight='bold')\n",
        "\n",
        "# Subplot 1: Datos originales\n",
        "axes[0,0].hist(original_data, bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "axes[0,0].set_title('📋 Datos Originales', fontweight='bold')\n",
        "axes[0,0].set_xlabel(sample_feature)\n",
        "axes[0,0].set_ylabel('Frecuencia')\n",
        "axes[0,0].grid(True, alpha=0.3)\n",
        "\n",
        "# Estadísticas originales\n",
        "mean_orig = original_data.mean()\n",
        "std_orig = original_data.std()\n",
        "axes[0,0].axvline(mean_orig, color='red', linestyle='--', \n",
        "                  label=f'Media: {mean_orig:.2f}')\n",
        "axes[0,0].axvline(mean_orig + std_orig, color='orange', linestyle='--', alpha=0.7,\n",
        "                  label=f'±1 STD: {std_orig:.2f}')\n",
        "axes[0,0].axvline(mean_orig - std_orig, color='orange', linestyle='--', alpha=0.7)\n",
        "axes[0,0].legend()\n",
        "\n",
        "# Obtener la posición de la variable Age en los datos transformados\n",
        "age_position = numeric_features.index(sample_feature)\n",
        "\n",
        "# Subplot 2: StandardScaler\n",
        "standard_scaled_data = processed_datasets['StandardScaler_OneHot']['X_train'][:, age_position]\n",
        "axes[0,1].hist(standard_scaled_data, bins=20, alpha=0.7, color='lightgreen', edgecolor='black')\n",
        "axes[0,1].set_title('🔧 StandardScaler', fontweight='bold')\n",
        "axes[0,1].set_xlabel(f'{sample_feature} (Estandarizado)')\n",
        "axes[0,1].set_ylabel('Frecuencia')\n",
        "axes[0,1].grid(True, alpha=0.3)\n",
        "\n",
        "# Estadísticas estandarizadas\n",
        "mean_std = standard_scaled_data.mean()\n",
        "std_std = standard_scaled_data.std()\n",
        "axes[0,1].axvline(mean_std, color='red', linestyle='--', \n",
        "                  label=f'Media: {mean_std:.2f}')\n",
        "axes[0,1].axvline(mean_std + std_std, color='orange', linestyle='--', alpha=0.7,\n",
        "                  label=f'±1 STD: {std_std:.2f}')\n",
        "axes[0,1].axvline(mean_std - std_std, color='orange', linestyle='--', alpha=0.7)\n",
        "axes[0,1].legend()\n",
        "\n",
        "# Subplot 3: MinMaxScaler\n",
        "minmax_scaled_data = processed_datasets['MinMaxScaler_OneHot']['X_train'][:, age_position]\n",
        "axes[1,0].hist(minmax_scaled_data, bins=20, alpha=0.7, color='lightcoral', edgecolor='black')\n",
        "axes[1,0].set_title('🔧 MinMaxScaler', fontweight='bold')\n",
        "axes[1,0].set_xlabel(f'{sample_feature} (Min-Max)')\n",
        "axes[1,0].set_ylabel('Frecuencia')\n",
        "axes[1,0].grid(True, alpha=0.3)\n",
        "\n",
        "# Estadísticas min-max\n",
        "mean_mm = minmax_scaled_data.mean()\n",
        "std_mm = minmax_scaled_data.std()\n",
        "axes[1,0].axvline(mean_mm, color='red', linestyle='--', \n",
        "                  label=f'Media: {mean_mm:.2f}')\n",
        "axes[1,0].axvline(0, color='green', linestyle='--', alpha=0.7, label='Min: 0')\n",
        "axes[1,0].axvline(1, color='green', linestyle='--', alpha=0.7, label='Max: 1')\n",
        "axes[1,0].legend()\n",
        "\n",
        "# Subplot 4: RobustScaler\n",
        "robust_scaled_data = processed_datasets['RobustScaler_Ordinal']['X_train'][:, age_position]\n",
        "axes[1,1].hist(robust_scaled_data, bins=20, alpha=0.7, color='lightsalmon', edgecolor='black')\n",
        "axes[1,1].set_title('🔧 RobustScaler', fontweight='bold')\n",
        "axes[1,1].set_xlabel(f'{sample_feature} (Robust)')\n",
        "axes[1,1].set_ylabel('Frecuencia')\n",
        "axes[1,1].grid(True, alpha=0.3)\n",
        "\n",
        "# Estadísticas robust\n",
        "mean_rb = robust_scaled_data.mean()\n",
        "std_rb = robust_scaled_data.std()\n",
        "axes[1,1].axvline(mean_rb, color='red', linestyle='--', \n",
        "                  label=f'Media: {mean_rb:.2f}')\n",
        "axes[1,1].axvline(0, color='green', linestyle='--', alpha=0.7, label='Mediana: 0')\n",
        "axes[1,1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Tabla comparativa de estadísticas\n",
        "print(f\"\\n📊 ESTADÍSTICAS COMPARATIVAS - Variable: {sample_feature}\")\n",
        "print(\"=\"*60)\n",
        "print(f\"{'Transformación':<20} {'Media':<10} {'Std':<10} {'Min':<10} {'Max':<10}\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# Datos originales\n",
        "print(f\"{'Original':<20} {original_data.mean():<10.3f} {original_data.std():<10.3f} \"\n",
        "      f\"{original_data.min():<10.3f} {original_data.max():<10.3f}\")\n",
        "\n",
        "# StandardScaler\n",
        "print(f\"{'StandardScaler':<20} {standard_scaled_data.mean():<10.3f} {standard_scaled_data.std():<10.3f} \"\n",
        "      f\"{standard_scaled_data.min():<10.3f} {standard_scaled_data.max():<10.3f}\")\n",
        "\n",
        "# MinMaxScaler\n",
        "print(f\"{'MinMaxScaler':<20} {minmax_scaled_data.mean():<10.3f} {minmax_scaled_data.std():<10.3f} \"\n",
        "      f\"{minmax_scaled_data.min():<10.3f} {minmax_scaled_data.max():<10.3f}\")\n",
        "\n",
        "# RobustScaler\n",
        "print(f\"{'RobustScaler':<20} {robust_scaled_data.mean():<10.3f} {robust_scaled_data.std():<10.3f} \"\n",
        "      f\"{robust_scaled_data.min():<10.3f} {robust_scaled_data.max():<10.3f}\")\n",
        "\n",
        "print(f\"\\n💡 OBSERVACIONES:\")\n",
        "print(\"   🎯 StandardScaler: Media ≈ 0, Std ≈ 1\")\n",
        "print(\"   🎯 MinMaxScaler: Rango [0, 1]\")\n",
        "print(\"   🎯 RobustScaler: Mediana ≈ 0, resistente a outliers\")\n",
        "\n",
        "# Guardar datasets procesados para el siguiente notebook\n",
        "joblib.dump(processed_datasets, '../models/processed_datasets.joblib')\n",
        "print(f\"\\n💾 Datasets procesados guardados: ../models/processed_datasets.joblib\")\n",
        "print(f\"✅ Listo para el benchmarking de modelos de machine learning\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

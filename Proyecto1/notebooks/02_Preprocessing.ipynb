{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Proyecto 1: Preprocesamiento de Datos\n",
        "## PreparaciÃ³n de Datos para Machine Learning\n",
        "\n",
        "**Objetivo**: Implementar pipelines de preprocesamiento usando ColumnTransformer y tÃ©cnicas avanzadas de transformaciÃ³n de datos para preparar el dataset para modelado de machine learning.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ImportaciÃ³n de bibliotecas para preprocesamiento\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import (StandardScaler, MinMaxScaler, RobustScaler, \n",
        "                                 LabelEncoder, OneHotEncoder, OrdinalEncoder)\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "import joblib\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"âœ… Bibliotecas de preprocesamiento importadas correctamente\")\n",
        "print(\"ğŸ”§ Listo para implementar pipelines de transformaciÃ³n\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. Carga de Datos Procesados del EDA\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cargar el dataset procesado del EDA\n",
        "try:\n",
        "    df = pd.read_csv('../data_processed_eda.csv')\n",
        "    print(\"âœ… Dataset del EDA cargado correctamente\")\n",
        "except FileNotFoundError:\n",
        "    # Cargar dataset original y aplicar transformaciones del EDA\n",
        "    df = pd.read_csv('../retail_sales_dataset.csv')\n",
        "    \n",
        "    # Convertir la columna Date a datetime\n",
        "    df['Date'] = pd.to_datetime(df['Date'])\n",
        "    \n",
        "    # Extraer caracterÃ­sticas temporales\n",
        "    df['Year'] = df['Date'].dt.year\n",
        "    df['Month'] = df['Date'].dt.month\n",
        "    df['Month_Name'] = df['Date'].dt.strftime('%B')\n",
        "    df['Day'] = df['Date'].dt.day\n",
        "    df['DayOfWeek'] = df['Date'].dt.dayofweek\n",
        "    df['DayName'] = df['Date'].dt.strftime('%A')\n",
        "    df['Quarter'] = df['Date'].dt.quarter\n",
        "    \n",
        "    # ClasificaciÃ³n de ventas\n",
        "    def clasificador_ventas(amount):\n",
        "        if amount >= 1000:\n",
        "            return 'Alta'\n",
        "        elif amount >= 300:\n",
        "            return 'Media'\n",
        "        else:\n",
        "            return 'Baja'\n",
        "    \n",
        "    df['Sales_Category'] = df['Total Amount'].apply(clasificador_ventas)\n",
        "    \n",
        "    # ClasificaciÃ³n de edad\n",
        "    def clasificador_edad(age):\n",
        "        if age >= 50:\n",
        "            return 'Adulto Mayor'\n",
        "        elif age >= 30 and age < 50:\n",
        "            return 'Adulto'\n",
        "        else:\n",
        "            return 'Joven'\n",
        "    \n",
        "    df['Age_Group'] = df['Age'].apply(clasificador_edad)\n",
        "    \n",
        "    # NormalizaciÃ³n Min-Max\n",
        "    min_sales = df['Total Amount'].min()\n",
        "    max_sales = df['Total Amount'].max()\n",
        "    df['Total_Amount_Normalized'] = (df['Total Amount'] - min_sales) / (max_sales - min_sales)\n",
        "    \n",
        "    print(\"âœ… Transformaciones del EDA aplicadas al dataset original\")\n",
        "\n",
        "print(f\"ğŸ“Š Dimensiones del dataset: {df.shape}\")\n",
        "print(f\"ğŸ“‹ Columnas disponibles: {len(df.columns)}\")\n",
        "print(\"\\nğŸ” Primeras filas:\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. DefiniciÃ³n de Variables y Target\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Definir diferentes problemas de machine learning que podemos resolver\n",
        "\n",
        "print(\"ğŸ¯ DEFINICIÃ“N DE PROBLEMAS DE MACHINE LEARNING\")\n",
        "print(\"=\"*55)\n",
        "\n",
        "# Problema 1: ClasificaciÃ³n de categorÃ­as de venta (Sales_Category)\n",
        "# Target: Sales_Category (Alta, Media, Baja)\n",
        "print(\"\\nğŸ“Š Problema 1: ClasificaciÃ³n de CategorÃ­as de Venta\")\n",
        "print(\"   ğŸ¯ Target: Sales_Category (Alta, Media, Baja)\")\n",
        "print(\"   ğŸ“ˆ Tipo: ClasificaciÃ³n multiclase\")\n",
        "\n",
        "# Problema 2: RegresiÃ³n para predecir Total Amount\n",
        "print(\"\\nğŸ’° Problema 2: PredicciÃ³n del Monto Total de Venta\")\n",
        "print(\"   ğŸ¯ Target: Total Amount (variable continua)\")\n",
        "print(\"   ğŸ“ˆ Tipo: RegresiÃ³n\")\n",
        "\n",
        "# Problema 3: ClasificaciÃ³n binaria de ventas altas\n",
        "# Crear variable binaria para ventas >= 1000\n",
        "df['High_Sales'] = (df['Total Amount'] >= 1000).astype(int)\n",
        "print(\"\\nğŸš€ Problema 3: PredicciÃ³n de Ventas Altas (Binary)\")\n",
        "print(\"   ğŸ¯ Target: High_Sales (1: >= $1000, 0: < $1000)\")\n",
        "print(\"   ğŸ“ˆ Tipo: ClasificaciÃ³n binaria\")\n",
        "\n",
        "# Seleccionar el problema principal: ClasificaciÃ³n de Sales_Category\n",
        "target_variable = 'Sales_Category'\n",
        "print(f\"\\nâœ… PROBLEMA SELECCIONADO: ClasificaciÃ³n de {target_variable}\")\n",
        "\n",
        "# Definir features para el modelo\n",
        "# Excluir variables que no deben usarse como features\n",
        "exclude_columns = [\n",
        "    'Transaction ID', 'Customer ID', 'Date',  # IDs y fechas\n",
        "    'Total Amount', 'Total_Amount_Normalized',  # Target leak\n",
        "    'Sales_Category', 'High_Sales'  # Variables target\n",
        "]\n",
        "\n",
        "features_all = [col for col in df.columns if col not in exclude_columns]\n",
        "print(f\"\\nğŸ“‹ FEATURES DISPONIBLES ({len(features_all)}):\")\n",
        "for i, feature in enumerate(features_all, 1):\n",
        "    print(f\"   {i:2d}. {feature}\")\n",
        "\n",
        "# Separar features por tipo\n",
        "numeric_features = df[features_all].select_dtypes(include=[np.number]).columns.tolist()\n",
        "categorical_features = df[features_all].select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "print(f\"\\nğŸ”¢ FEATURES NUMÃ‰RICAS ({len(numeric_features)}):\")\n",
        "for feature in numeric_features:\n",
        "    print(f\"   â€¢ {feature}\")\n",
        "\n",
        "print(f\"\\nğŸ·ï¸ FEATURES CATEGÃ“RICAS ({len(categorical_features)}):\")\n",
        "for feature in categorical_features:\n",
        "    unique_values = df[feature].nunique()\n",
        "    print(f\"   â€¢ {feature} ({unique_values} valores Ãºnicos)\")\n",
        "\n",
        "# Verificar distribuciÃ³n del target\n",
        "print(f\"\\nğŸ¯ DISTRIBUCIÃ“N DE LA VARIABLE TARGET ({target_variable}):\")\n",
        "target_distribution = df[target_variable].value_counts()\n",
        "for category, count in target_distribution.items():\n",
        "    percentage = (count / len(df)) * 100\n",
        "    print(f\"   ğŸ“Š {category}: {count} ({percentage:.1f}%)\")\"\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. ImplementaciÃ³n de Pipelines con ColumnTransformer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear diferentes pipelines de preprocesamiento\n",
        "\n",
        "print(\"ğŸ”§ CREACIÃ“N DE PIPELINES DE PREPROCESAMIENTO\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Pipeline 1: StandardScaler para numÃ©ricas + OneHotEncoder para categÃ³ricas\n",
        "numeric_pipeline_1 = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_pipeline_1 = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "preprocessor_1 = ColumnTransformer([\n",
        "    ('numeric', numeric_pipeline_1, numeric_features),\n",
        "    ('categorical', categorical_pipeline_1, categorical_features)\n",
        "])\n",
        "\n",
        "print(\"âœ… Pipeline 1: StandardScaler + OneHotEncoder\")\n",
        "\n",
        "# Pipeline 2: MinMaxScaler para numÃ©ricas + OneHotEncoder para categÃ³ricas\n",
        "numeric_pipeline_2 = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', MinMaxScaler())\n",
        "])\n",
        "\n",
        "preprocessor_2 = ColumnTransformer([\n",
        "    ('numeric', numeric_pipeline_2, numeric_features),\n",
        "    ('categorical', categorical_pipeline_1, categorical_features)\n",
        "])\n",
        "\n",
        "print(\"âœ… Pipeline 2: MinMaxScaler + OneHotEncoder\")\n",
        "\n",
        "# Pipeline 3: RobustScaler para numÃ©ricas + OrdinalEncoder para categÃ³ricas\n",
        "categorical_pipeline_3 = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('ordinal', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))\n",
        "])\n",
        "\n",
        "numeric_pipeline_3 = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', RobustScaler())\n",
        "])\n",
        "\n",
        "preprocessor_3 = ColumnTransformer([\n",
        "    ('numeric', numeric_pipeline_3, numeric_features),\n",
        "    ('categorical', categorical_pipeline_3, categorical_features)\n",
        "])\n",
        "\n",
        "print(\"âœ… Pipeline 3: RobustScaler + OrdinalEncoder\")\n",
        "\n",
        "# Crear diccionario de preprocessors\n",
        "preprocessors = {\n",
        "    'StandardScaler_OneHot': preprocessor_1,\n",
        "    'MinMaxScaler_OneHot': preprocessor_2,\n",
        "    'RobustScaler_Ordinal': preprocessor_3\n",
        "}\n",
        "\n",
        "print(f\"\\nğŸ“Š Total de pipelines creados: {len(preprocessors)}\")\n",
        "print(\"ğŸ¯ Cada pipeline maneja automÃ¡ticamente:\")\n",
        "print(\"   â€¢ ImputaciÃ³n de valores faltantes\")\n",
        "print(\"   â€¢ Escalado de variables numÃ©ricas\")\n",
        "print(\"   â€¢ CodificaciÃ³n de variables categÃ³ricas\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 4. DivisiÃ³n de Datos y AplicaciÃ³n de Transformaciones\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preparar datos para machine learning\n",
        "X = df[features_all]\n",
        "y = df[target_variable]\n",
        "\n",
        "# Codificar la variable target\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "print(\"ğŸ“Š PREPARACIÃ“N DE DATOS PARA MACHINE LEARNING\")\n",
        "print(\"=\"*50)\n",
        "print(f\"âœ… Features (X): {X.shape}\")\n",
        "print(f\"âœ… Target (y): {y.shape}\")\n",
        "print(f\"ğŸ“‹ Clases del target: {list(label_encoder.classes_)}\")\n",
        "print(f\"ğŸ”¢ Target codificado: {dict(zip(label_encoder.classes_, range(len(label_encoder.classes_))))}\")\n",
        "\n",
        "# DivisiÃ³n en conjuntos de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y_encoded, \n",
        "    test_size=0.2, \n",
        "    random_state=42, \n",
        "    stratify=y_encoded\n",
        ")\n",
        "\n",
        "print(f\"\\nğŸ“Š DIVISIÃ“N DE DATOS:\")\n",
        "print(f\"   ğŸ‹ï¸ Entrenamiento: {X_train.shape[0]} muestras ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
        "print(f\"   ğŸ§ª Prueba: {X_test.shape[0]} muestras ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
        "\n",
        "# Verificar distribuciÃ³n de clases en cada conjunto\n",
        "train_distribution = pd.Series(y_train).value_counts().sort_index()\n",
        "test_distribution = pd.Series(y_test).value_counts().sort_index()\n",
        "\n",
        "print(f\"\\nğŸ¯ DISTRIBUCIÃ“N DE CLASES:\")\n",
        "print(\"Conjunto de Entrenamiento:\")\n",
        "for class_idx, count in train_distribution.items():\n",
        "    class_name = label_encoder.inverse_transform([class_idx])[0]\n",
        "    print(f\"   ğŸ“Š {class_name}: {count} ({count/len(y_train)*100:.1f}%)\")\n",
        "\n",
        "print(\"Conjunto de Prueba:\")\n",
        "for class_idx, count in test_distribution.items():\n",
        "    class_name = label_encoder.inverse_transform([class_idx])[0]\n",
        "    print(f\"   ğŸ“Š {class_name}: {count} ({count/len(y_test)*100:.1f}%)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Aplicar cada pipeline de preprocesamiento y guardar resultados\n",
        "processed_datasets = {}\n",
        "\n",
        "print(\"ğŸ”§ APLICACIÃ“N DE PIPELINES DE PREPROCESAMIENTO\")\n",
        "print(\"=\"*55)\n",
        "\n",
        "for name, preprocessor in preprocessors.items():\n",
        "    print(f\"\\nğŸš€ Aplicando pipeline: {name}\")\n",
        "    \n",
        "    # Ajustar y transformar datos de entrenamiento\n",
        "    X_train_processed = preprocessor.fit_transform(X_train)\n",
        "    \n",
        "    # Transformar datos de prueba (solo transform, no fit)\n",
        "    X_test_processed = preprocessor.transform(X_test)\n",
        "    \n",
        "    # Guardar resultados\n",
        "    processed_datasets[name] = {\n",
        "        'X_train': X_train_processed,\n",
        "        'X_test': X_test_processed,\n",
        "        'y_train': y_train,\n",
        "        'y_test': y_test,\n",
        "        'preprocessor': preprocessor,\n",
        "        'label_encoder': label_encoder\n",
        "    }\n",
        "    \n",
        "    print(f\"   âœ… Entrenamiento: {X_train_processed.shape}\")\n",
        "    print(f\"   âœ… Prueba: {X_test_processed.shape}\")\n",
        "    \n",
        "    # Mostrar informaciÃ³n sobre las transformaciones aplicadas\n",
        "    if hasattr(preprocessor, 'transformers_'):\n",
        "        for transformer_name, transformer, columns in preprocessor.transformers_:\n",
        "            if transformer_name != 'remainder':\n",
        "                print(f\"   ğŸ”¹ {transformer_name}: {len(columns)} columnas\")\n",
        "\n",
        "print(f\"\\nğŸ“Š RESUMEN DE DATASETS PROCESADOS:\")\n",
        "for name in processed_datasets.keys():\n",
        "    print(f\"   âœ… {name}: Listo para machine learning\")\n",
        "\n",
        "# Guardar los preprocessors para uso futuro\n",
        "import os\n",
        "os.makedirs('../models', exist_ok=True)\n",
        "\n",
        "for name, preprocessor in preprocessors.items():\n",
        "    filename = f'../models/preprocessor_{name}.joblib'\n",
        "    joblib.dump(preprocessor, filename)\n",
        "    print(f\"ğŸ’¾ Preprocessor guardado: {filename}\")\n",
        "\n",
        "# Guardar label encoder\n",
        "joblib.dump(label_encoder, '../models/label_encoder.joblib')\n",
        "print(f\"ğŸ’¾ Label encoder guardado: ../models/label_encoder.joblib\")\n",
        "\n",
        "print(f\"\\nâœ… PREPROCESAMIENTO COMPLETADO\")\n",
        "print(f\"ğŸ¯ {len(processed_datasets)} datasets listos para benchmarking de modelos\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 5. AnÃ¡lisis de las Transformaciones Aplicadas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analizar el impacto de diferentes transformaciones\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"ğŸ“Š ANÃLISIS DEL IMPACTO DE LAS TRANSFORMACIONES\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Seleccionar una variable numÃ©rica para comparar transformaciones\n",
        "sample_feature = 'Age'  # Usar Age como ejemplo\n",
        "original_data = X_train[sample_feature]\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "fig.suptitle(f'ğŸ“Š COMPARACIÃ“N DE TRANSFORMACIONES - Variable: {sample_feature}', \n",
        "             fontsize=16, fontweight='bold')\n",
        "\n",
        "# Subplot 1: Datos originales\n",
        "axes[0,0].hist(original_data, bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "axes[0,0].set_title('ğŸ“‹ Datos Originales', fontweight='bold')\n",
        "axes[0,0].set_xlabel(sample_feature)\n",
        "axes[0,0].set_ylabel('Frecuencia')\n",
        "axes[0,0].grid(True, alpha=0.3)\n",
        "\n",
        "# EstadÃ­sticas originales\n",
        "mean_orig = original_data.mean()\n",
        "std_orig = original_data.std()\n",
        "axes[0,0].axvline(mean_orig, color='red', linestyle='--', \n",
        "                  label=f'Media: {mean_orig:.2f}')\n",
        "axes[0,0].axvline(mean_orig + std_orig, color='orange', linestyle='--', alpha=0.7,\n",
        "                  label=f'Â±1 STD: {std_orig:.2f}')\n",
        "axes[0,0].axvline(mean_orig - std_orig, color='orange', linestyle='--', alpha=0.7)\n",
        "axes[0,0].legend()\n",
        "\n",
        "# Obtener la posiciÃ³n de la variable Age en los datos transformados\n",
        "age_position = numeric_features.index(sample_feature)\n",
        "\n",
        "# Subplot 2: StandardScaler\n",
        "standard_scaled_data = processed_datasets['StandardScaler_OneHot']['X_train'][:, age_position]\n",
        "axes[0,1].hist(standard_scaled_data, bins=20, alpha=0.7, color='lightgreen', edgecolor='black')\n",
        "axes[0,1].set_title('ğŸ”§ StandardScaler', fontweight='bold')\n",
        "axes[0,1].set_xlabel(f'{sample_feature} (Estandarizado)')\n",
        "axes[0,1].set_ylabel('Frecuencia')\n",
        "axes[0,1].grid(True, alpha=0.3)\n",
        "\n",
        "# EstadÃ­sticas estandarizadas\n",
        "mean_std = standard_scaled_data.mean()\n",
        "std_std = standard_scaled_data.std()\n",
        "axes[0,1].axvline(mean_std, color='red', linestyle='--', \n",
        "                  label=f'Media: {mean_std:.2f}')\n",
        "axes[0,1].axvline(mean_std + std_std, color='orange', linestyle='--', alpha=0.7,\n",
        "                  label=f'Â±1 STD: {std_std:.2f}')\n",
        "axes[0,1].axvline(mean_std - std_std, color='orange', linestyle='--', alpha=0.7)\n",
        "axes[0,1].legend()\n",
        "\n",
        "# Subplot 3: MinMaxScaler\n",
        "minmax_scaled_data = processed_datasets['MinMaxScaler_OneHot']['X_train'][:, age_position]\n",
        "axes[1,0].hist(minmax_scaled_data, bins=20, alpha=0.7, color='lightcoral', edgecolor='black')\n",
        "axes[1,0].set_title('ğŸ”§ MinMaxScaler', fontweight='bold')\n",
        "axes[1,0].set_xlabel(f'{sample_feature} (Min-Max)')\n",
        "axes[1,0].set_ylabel('Frecuencia')\n",
        "axes[1,0].grid(True, alpha=0.3)\n",
        "\n",
        "# EstadÃ­sticas min-max\n",
        "mean_mm = minmax_scaled_data.mean()\n",
        "std_mm = minmax_scaled_data.std()\n",
        "axes[1,0].axvline(mean_mm, color='red', linestyle='--', \n",
        "                  label=f'Media: {mean_mm:.2f}')\n",
        "axes[1,0].axvline(0, color='green', linestyle='--', alpha=0.7, label='Min: 0')\n",
        "axes[1,0].axvline(1, color='green', linestyle='--', alpha=0.7, label='Max: 1')\n",
        "axes[1,0].legend()\n",
        "\n",
        "# Subplot 4: RobustScaler\n",
        "robust_scaled_data = processed_datasets['RobustScaler_Ordinal']['X_train'][:, age_position]\n",
        "axes[1,1].hist(robust_scaled_data, bins=20, alpha=0.7, color='lightsalmon', edgecolor='black')\n",
        "axes[1,1].set_title('ğŸ”§ RobustScaler', fontweight='bold')\n",
        "axes[1,1].set_xlabel(f'{sample_feature} (Robust)')\n",
        "axes[1,1].set_ylabel('Frecuencia')\n",
        "axes[1,1].grid(True, alpha=0.3)\n",
        "\n",
        "# EstadÃ­sticas robust\n",
        "mean_rb = robust_scaled_data.mean()\n",
        "std_rb = robust_scaled_data.std()\n",
        "axes[1,1].axvline(mean_rb, color='red', linestyle='--', \n",
        "                  label=f'Media: {mean_rb:.2f}')\n",
        "axes[1,1].axvline(0, color='green', linestyle='--', alpha=0.7, label='Mediana: 0')\n",
        "axes[1,1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Tabla comparativa de estadÃ­sticas\n",
        "print(f\"\\nğŸ“Š ESTADÃSTICAS COMPARATIVAS - Variable: {sample_feature}\")\n",
        "print(\"=\"*60)\n",
        "print(f\"{'TransformaciÃ³n':<20} {'Media':<10} {'Std':<10} {'Min':<10} {'Max':<10}\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# Datos originales\n",
        "print(f\"{'Original':<20} {original_data.mean():<10.3f} {original_data.std():<10.3f} \"\n",
        "      f\"{original_data.min():<10.3f} {original_data.max():<10.3f}\")\n",
        "\n",
        "# StandardScaler\n",
        "print(f\"{'StandardScaler':<20} {standard_scaled_data.mean():<10.3f} {standard_scaled_data.std():<10.3f} \"\n",
        "      f\"{standard_scaled_data.min():<10.3f} {standard_scaled_data.max():<10.3f}\")\n",
        "\n",
        "# MinMaxScaler\n",
        "print(f\"{'MinMaxScaler':<20} {minmax_scaled_data.mean():<10.3f} {minmax_scaled_data.std():<10.3f} \"\n",
        "      f\"{minmax_scaled_data.min():<10.3f} {minmax_scaled_data.max():<10.3f}\")\n",
        "\n",
        "# RobustScaler\n",
        "print(f\"{'RobustScaler':<20} {robust_scaled_data.mean():<10.3f} {robust_scaled_data.std():<10.3f} \"\n",
        "      f\"{robust_scaled_data.min():<10.3f} {robust_scaled_data.max():<10.3f}\")\n",
        "\n",
        "print(f\"\\nğŸ’¡ OBSERVACIONES:\")\n",
        "print(\"   ğŸ¯ StandardScaler: Media â‰ˆ 0, Std â‰ˆ 1\")\n",
        "print(\"   ğŸ¯ MinMaxScaler: Rango [0, 1]\")\n",
        "print(\"   ğŸ¯ RobustScaler: Mediana â‰ˆ 0, resistente a outliers\")\n",
        "\n",
        "# Guardar datasets procesados para el siguiente notebook\n",
        "joblib.dump(processed_datasets, '../models/processed_datasets.joblib')\n",
        "print(f\"\\nğŸ’¾ Datasets procesados guardados: ../models/processed_datasets.joblib\")\n",
        "print(f\"âœ… Listo para el benchmarking de modelos de machine learning\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
